---
sidebar_position: 1000
---

# Bibliography

The page contains an organized list of all papers used by this course. 
The papers are organized by topic.

ðŸ”µ = Paper directly cited in this course. Other papers have informed my understanding of the topic.

## Prompt Engineering Strategies

#### Chain of Thought(@wei2022chain) ðŸ”µ

#### Zero Shot Chain of Thought(@kojima2022large) ðŸ”µ

#### Self Consistency(@wang2022selfconsistency) ðŸ”µ

#### What Makes Good In-Context Examples for GPT-3?(@liu2021makes) ðŸ”µ

#### Generated Knowledge(@liu2021generated) ðŸ”µ

#### Rethinking the role of demonstrations(@min2022rethinking) ðŸ”µ

#### Scratchpads(@nye2021work)

#### Maieutic Prompting(@jung2022maieutic)

## Reliability

#### The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning(@ye2022unreliability) ðŸ”µ

#### Prompting GPT-3 to be reliable(@si2022prompting)

#### Diverse Prompts(@li2022advance)

#### Problems with biases(@zhao2021calibrate)

#### Enhanced Self Consistency(@mitchell2022enhancing)

## Automated Prompt Engineering

#### AutoPrompt(@shin2020autoprompt) ðŸ”µ

#### Automatic Prompt Engineer(@zhou2022large)

## Models

### Language Models

#### GPT-3(@brown2020language) ðŸ”µ

#### GPT-3 Instruct(@ouyang2022training) ðŸ”µ

#### PaLM(@chowdhery2022palm) ðŸ”µ

#### BLOOM(@scao2022bloom) ðŸ”µ

#### Jurassic 1(@lieberjurassic) ðŸ”µ

### Image Models

#### Stable Diffusion(@rombach2021highresolution) ðŸ”µ

#### DALLE(@ramesh2022hierarchical) ðŸ”µ

## Soft Prompting

#### Soft Prompting(@lester2021power)

#### Interpretable Discretized Soft Prompts(@khashabi2021prompt)

## Datasets

#### GSM8K(@cobbe2021training) ðŸ”µ

#### HotPotQA(@yang2018hotpotqa) ðŸ”µ

#### Fever(@thorne2018fever) ðŸ”µ

## Image Prompt Engineering

#### Taxonomy of prompt modifiers(@oppenlaender2022taxonomy)

#### DiffusionDB(@wang2022diffusiondb)

## Prompt Engineering IDEs

#### Prompt IDE(@strobelt2022promptide) ðŸ”µ

#### Prompt Source(@bach2022promptsource) ðŸ”µ

#### PromptChainer(@wu2022promptchainer) ðŸ”µ

## Applied Prompt Engineering

#### Language Model Cascades(@dohan2022language)

#### MRKL(@karpas2022mrkl) ðŸ”µ

#### ReAct(@yao2022react) ðŸ”µ

#### PAL: Program-aided Language Models(@gao2022pal)

## User Interface Design

#### Design Guidelines for Prompt Engineering Text-to-Image Generative Models(@liu2022design)

## Prompt Injection

#### Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods(@crothers2022machine) ðŸ”µ

#### Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples(@branch2022evaluating) ðŸ”µ

#### Prompt injection attacks against GPT-3(@simon2022inject) ðŸ”µ

#### Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions(@goodside2022inject) ðŸ”µ

#### adversarial-prompts(@chase2021adversarial) ðŸ”µ

#### GPT-3 Prompt Injection Defenses(@goodside2021gpt) ðŸ”µ

## Surveys

#### Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing(@liu2021pretrain)

#### PromptPapers(@ning2022papers)

## Miscl

#### Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models(@bursztyn2022learning)

#### Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks(@wang2022supernaturalinstructions)

#### Making Pre-trained Language Models Better Few-shot Learners(@gao2021making)

#### Grounding with search results(@livin2022large)

#### How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models(@dang2022prompt)

#### On Measuring Social Biases in Prompt-Based Multi-Task Learning(@akyrek2022measuring)