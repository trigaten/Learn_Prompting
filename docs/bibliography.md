---
sidebar_position: 1000
---

# ğŸ“š Bibliografie

StrÃ¡nka obsahuje uspoÅ™Ã¡danÃ½ seznam vÅ¡ech dokumentÅ¯ pouÅ¾itÃ½ch v tomto kurzu.
Dokumenty jsou uspoÅ™Ã¡dÃ¡ny podle tÃ©mat.

**Chcete-li citovat tento kurz, pouÅ¾ijte uvedenou citaci v ÃºloÅ¾iÅ¡ti Github**.

ğŸ”µ = PÅ™Ã­spÄ›vek pÅ™Ã­mo citovanÃ½ v tomto kurzu. OstatnÃ­ dokumenty pÅ™ispÄ›ly k mÃ©mu pochopenÃ­ tÃ©matu.

PoznÃ¡mka: vzhledem k tomu, Å¾e [ani dokument GPT-3, ani dokument GPT-3 Instruct neodpovÃ­dajÃ­ davinciho modelÅ¯m](https://twitter.com/janleike/status/1584618242756132864), snaÅ¾Ã­m se je nezobrazovat
je jako takovÃ© citovat.

## PromptnÃ­ inÅ¾enÃ½rskÃ© strategie

#### MyÅ¡lenkovÃ½ Å™etÄ›zec(@wei2022chain) ğŸ”µ

#### Zero Shot Chain of Thought(@kojima2022large) ğŸ”µ

#### SebedÅ¯slednost(@wang2022selfconsistency) ğŸ”µ

#### Co dÄ›lÃ¡ dobrÃ© pÅ™Ã­klady v kontextu pro GPT-3?(@liu2021makes) ğŸ”µ

### Ask-Me-Anything Prompting(@arora2022ama) ğŸ”µ

#### GenerovanÃ© znalosti(@liu2021generated) ğŸ”µ

#### RecitacÃ­ rozÅ¡Ã­Å™enÃ© jazykovÃ© modely(@sun2022recitationaugmented) ğŸ”µ

#### PÅ™ehodnocenÃ­ role demonstracÃ­(@min2022rethinking) ğŸ”µ

#### Scratchpady(@nye2021work)

#### Maieutic Prompting(@jung2022maieutic)

#### STaR(@zelikman2022star)

#### Least to Most(@zhou2022leasttomost) ğŸ”µ

#### PÅ™erÃ¡movÃ¡nÃ­ instrukÄnÃ­ch vÃ½zev do jazyka GPTk(@mishra2022reframing) ğŸ”µ

#### TurkingÅ¯v test: RozumÃ­ jazykovÃ© modely instrukcÃ­m?(@efrat2020turking) ğŸ”µ

## Spolehlivost

#### MathPrompter(@imani2023mathprompter) ğŸ”µ

#### Nespolehlivost vysvÄ›tlenÃ­ v nÄ›kolikastrÃ¡nkovÃ© promptingovÃ© metodÄ› pro textovÃ© uvaÅ¾ovÃ¡nÃ­(@ye2022unreliability) ğŸ”µ

#### PromptovÃ¡nÃ­ GPT-3, aby bylo spolehlivÃ©(@si2022prompting)

#### RÅ¯znorodÃ© podnÄ›ty(@li2022advance) ğŸ”µ

#### Kalibrace pÅ™ed pouÅ¾itÃ­m: ZlepÅ¡enÃ­ vÃ½konu jazykovÃ½ch modelÅ¯ s nÄ›kolika snÃ­mky(@zhao2021calibrate) ğŸ”µ

#### ZvÃ½Å¡enÃ¡ vlastnÃ­ konzistence(@mitchell2022enhancing)

#### Bias and Toxicity in Zero-Shot CoT(@shaikh2022second) ğŸ”µ

#### KonstituÄnÃ­ UI: NeÅ¡kodnost ze zpÄ›tnÃ© vazby UI (@bai2022constitutional) ğŸ”µ

#### KompoziÄnÃ­ generalizace - SCAN(@lake2018scan)

## AutomatizovanÃ© inÅ¾enÃ½rstvÃ­ promptÅ¯

#### AutoPrompt (@shin2020autoprompt) ğŸ”µ

#### Automatic Prompt Engineer(@zhou2022large)

## Modely

### JazykovÃ© modely

#### GPT-3(@brown2020language) ğŸ”µ

#### GPT-3 Instruct(@ouyang2022training) ğŸ”µ

#### PaLM(@chowdhery2022palm) ğŸ”µ

#### BLOOM(@scao2022bloom) ğŸ”µ

#### BLOOM+1 (vÃ­ce jazykÅ¯/ 0 vylepÅ¡enÃ­ snÃ­mkÅ¯)(@yong2022bloom1)

#### TechnickÃ¡ zprÃ¡va GPT-4(@openai2023gpt4) ğŸ”µ

#### Jurassic 1(@lieberjurassic) ğŸ”µ

#### GPT-J-6B(@wange2021gptj)

#### Roberta(@liu2019roberta)

### Image Models

#### StabilnÃ­ difÃºze(@rombach2021highresolution) ğŸ”µ

#### DALLE(@ramesh2022hierarchical) ğŸ”µ

## MÄ›kkÃ© promÃ­tÃ¡nÃ­

#### Soft Prompting(@lester2021power) ğŸ”µ

#### InterpretovatelnÃ© diskrÃ©tnÃ­ mÄ›kkÃ© promptovÃ¡nÃ­(@khashabi2021prompt) ğŸ”µ

## DatovÃ© sady

#### MultiArith(@roy-roth-2015-solving) ğŸ”µ

#### GSM8K(@cobbe2021training) ğŸ”µ

#### HotPotQA(@yang2018hotpotqa) ğŸ”µ

#### Fever(@thorne2018fever) ğŸ”µ

#### BBQ: RuÄnÄ› sestavenÃ½ benchmark pro zodpovÃ­dÃ¡nÃ­ otÃ¡zek(@parrish2021bbq) ğŸ”µ

## Image Prompt Engineering

#### Taxonomie modifikÃ¡torÅ¯ vÃ½zvy(@oppenlaender2022taxonomy)

#### DiffusionDB(@wang2022diffusiondb)

#### The DALLE 2 Prompt Book(@parsons2022dalleprompt) ğŸ”µ

#### Prompt Engineering for Text-Based Generative Art(@oppenlaender2022prompt) ğŸ”µ

#### Se sprÃ¡vnÃ½m promptem umÃ­ stabilnÃ­ difÃºze 2.0 ruce (@blake2022with) ğŸ”µ

#### Optimalizace vÃ½zev pro generovÃ¡nÃ­ textu na obrÃ¡zek(@hao2022optimizing)

## Prompt Engineering IDE

#### Prompt IDE(@strobelt2022promptide) ğŸ”µ

#### Prompt Source(@bach2022promptsource) ğŸ”µ

#### PromptChainer(@wu2022promptchainer) ğŸ”µ

#### PromptMaker(@jiang2022promptmaker) ğŸ”µ

## Tooling

#### LangChain(@Chase_LangChain_2022) ğŸ”µ

#### TextBox 2.0: Knihovna pro generovÃ¡nÃ­ textu s pÅ™edem natrÃ©novanÃ½mi jazykovÃ½mi modely(@tang2022textbox) ğŸ”µ

#### OpenPrompt: Open-source Framework for Prompt-learning(@ding2021openprompt) ğŸ”µ

#### GPT Index(@Liu_GPT_Index_2022) ğŸ”µ

## AplikovanÃ© promptnÃ­ inÅ¾enÃ½rstvÃ­

#### KaskÃ¡dy jazykovÃ½ch modelÅ¯(@dohan2022language)

#### MRKL(@karpas2022mrkl) ğŸ”µ

#### ReAct(@yao2022react) ğŸ”µ

#### PAL: Programem podporovanÃ© jazykovÃ© modely(@gao2022pal) ğŸ”µ

## NÃ¡vrh uÅ¾ivatelskÃ©ho rozhranÃ­

#### Pokyny pro nÃ¡vrh pro promptnÃ­ inÅ¾enÃ½rstvÃ­ Text-to-Image generativnÃ­ch modelÅ¯(@liu2022design)

## Prompt Injection

#### StrojovÄ› generovanÃ½ text: A Comprehensive Survey of Threat Models and Detection Methods(@crothers2022machine) ğŸ”µ

#### HodnocenÃ­ vnÃ­mavosti pÅ™edem vyÅ¡kolenÃ½ch jazykovÃ½ch modelÅ¯ prostÅ™ednictvÃ­m ruÄnÄ› vytvoÅ™enÃ½ch protivnÃ½ch pÅ™Ã­kladÅ¯(@branch2022evaluating) ğŸ”µ

#### VyuÅ¾itÃ­ programovÃ©ho chovÃ¡nÃ­ LLM: DvojÃ­ vyuÅ¾itÃ­ prostÅ™ednictvÃ­m standardnÃ­ch bezpeÄnostnÃ­ch ÃºtokÅ¯(@kang2023exploiting) ğŸ”µ
    
#### VÃ­ce, neÅ¾ jste si Å™ekli: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models(@greshake2023youve) ğŸ”µ

#### Ãštoky Prompt injection proti GPT-3(@simon2022inject) ğŸ”µ

#### ZneuÅ¾itÃ­ vÃ½zev GPT-3 pomocÃ­ Å¡kodlivÃ½ch vstupÅ¯, kterÃ© modelu naÅ™Ã­dÃ­ ignorovat jeho pÅ™edchozÃ­ pokyny(@goodside2022inject) ğŸ”µ

#### adversarial-prompts(@chase2021adversarial) ğŸ”µ

#### ChatGPT "DAN" (a dalÅ¡Ã­ "ÃºtÄ›ky z vÄ›zenÃ­")(@kiho2023chatgpt) ğŸ”µ

#### GPT-3 Prompt Injection Defenses(@goodside2021gpt) ğŸ”µ

#### MluvenÃ­ se stroji: promptnÃ­ inÅ¾enÃ½rstvÃ­ a injekce(@christoph2022talking)

#### ProzkoumÃ¡nÃ­ ÃºtokÅ¯ Prompt Injection(@selvi2022exploring) ğŸ”µ

#### PouÅ¾itÃ­ GPT-Eliezer proti ÃºtÄ›ku z vÄ›zenÃ­ ChatGPT(@armstrong2022using) ğŸ”µ

#### Microsoft Bing Chat Prompt(@kevinbing)

## Jailbreaking

#### Ignorovat pÅ™edchozÃ­ vÃ½zvu: ÃštoÄnÃ© techniky pro jazykovÃ© modely(@perez2022jailbreak)

#### PouÄenÃ­ o bezpeÄnosti a zneuÅ¾itÃ­ jazykovÃ½ch modelÅ¯(@brundage_2022)

#### Detekce toxicity pomocÃ­ generativnÃ­ho odvozovÃ¡nÃ­ na zÃ¡kladÄ› propozic(@wang2022jailbreak)

#### NovÃ© a vylepÅ¡enÃ© nÃ¡stroje pro moderovÃ¡nÃ­ obsahu(@markov_2022)

#### OpenAI API(@openai_api) ğŸ”µ

#### OpenAI ChatGPT(@openai_chatgpt) ğŸ”µ

#### ChatGPT 4 Tweet(@alice2022jailbreak) ğŸ”µ

#### HranÃ­ Tweet(@miguel2022jailbreak) ğŸ”µ

#### VÃ½zkum Tweet(@derek2022jailbreak) ğŸ”µ

#### PÅ™edstÃ­rÃ¡nÃ­ schopnostÃ­ Tweet(@nero2022jailbreak) ğŸ”µ

#### ZodpovÄ›dnost Tweet(@nick2022jailbreak) ğŸ”µ

#### Lynx Mode Tweet(@jonas2022jailbreak) ğŸ”µ

#### Sudo Mode Tweet(@sudo2022jailbreak) ğŸ”µ

#### Ignorovat pÅ™edchozÃ­ vÃ½zvu(@ignore_previous_prompt) ğŸ”µ

#### AktualizovanÃ© vÃ½zvy k ÃºtÄ›ku z vÄ›zenÃ­ (@AI_jailbreak) ğŸ”µ

## PrÅ¯zkumy

#### PÅ™edbÄ›Å¾nÃ½ trÃ©nink, vÃ½zva a pÅ™edvÃ­dÃ¡nÃ­: SystematickÃ½ pÅ™ehled metod promptovÃ¡nÃ­ pÅ™i zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka(@liu2021pretrain)

#### PromptPapers(@ning2022papers)

## GenerovÃ¡nÃ­ datovÃ½ch sad

#### Discovering Language Model Behaviors with Model-Written Evaluations(@perez2022discovering)

#### SelektivnÃ­ anotace dÄ›lÃ¡ z jazykovÃ½ch modelÅ¯ lepÅ¡Ã­ uÄitele s nÄ›kolika snÃ­mky(@su2022selective)

## Aplikace

#### Atlas: Few-shot Learning with Retrieval Augmented Language Models(@izacard2022atlas)

#### STRUDEL: StrukturovanÃ¡ sumarizace dialogÅ¯ pro porozumÄ›nÃ­ dialogÅ¯m(@wang2022strudel)

## HorkÃ¡ tÃ©mata

#### Auto-GPT(@richards2023)

#### Baby AGI(@nakajima2023)

#### AgentGPT(@reworkd2023)

## Miscl

#### PromptovÃ¡nÃ­ je programovÃ¡nÃ­: A Query Language For Large Language Models(@beurerkellner2022prompting)

#### ParalelnÃ­ kontextovÃ¡ okna zlepÅ¡ujÃ­ uÄenÃ­ velkÃ½ch jazykovÃ½ch modelÅ¯ v kontextu(@ratner2022parallel)

#### A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT(@white2023prompt) ğŸ”µ.

#### UÄenÃ­ sloÅ¾itÃ½ch Ãºloh pomocÃ­ kompoziÄnÃ­ho dolaÄovÃ¡nÃ­ jazykovÃ½ch modelÅ¯(@bursztyn2022learning)

#### NadpÅ™irozenÃ©instrukce: Generalizace prostÅ™ednictvÃ­m deklarativnÃ­ch instrukcÃ­ na vÃ­ce neÅ¾ 1600 ÃºlohÃ¡ch NLP(@wang2022supernaturalinstructions)

#### Making Pre-trained Language Models Better Few-shot Learners(@gao2021making)

#### Grounding with search results(@livin2022large)

#### Jak na prompt? PÅ™Ã­leÅ¾itosti a vÃ½zvy uÄenÃ­ s nulovÃ½m poÄtem snÃ­mkÅ¯ a nÄ›kolika snÃ­mky pro interakci mezi ÄlovÄ›kem a umÄ›lou inteligencÃ­ v kreativnÃ­ch aplikacÃ­ch generativnÃ­ch modelÅ¯(@dang2022prompt)

#### On Measuring Social Biases in Prompt-Based Multi-Task Learning(@akyrek2022measuring)

#### PsanÃ­ zÃ¡pletek z pÅ™edem vytrÃ©novanÃ½ch jazykovÃ½ch modelÅ¯(@jin2022plot) ğŸ”µ

#### StereoSet: MÄ›Å™enÃ­ stereotypnÃ­ho zkreslenÃ­ v pÅ™edtrÃ©novanÃ½ch jazykovÃ½ch modelech(@nadeem-etal-2021-stereoset)

#### PÅ™ehled halucinacÃ­ pÅ™i generovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka(@Ji_2022)

#### PÅ™Ã­klady(@2022examples)

#### Wordcraft(@yuan2022wordcraft)

#### PainPoints(@fadnavis2022pain)

#### SebeinstruktÃ¡Å¾: SladÄ›nÃ­ jazykovÃ©ho modelu se samostatnÄ› generovanÃ½mi instrukcemi(@wang2022selfinstruct)

#### Od obrÃ¡zkÅ¯ k textovÃ½m podnÄ›tÅ¯m: Zero-shot VQA with Frozen Large Language Models(@guo2022images)

#### Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference(@schick2020exploiting)

### Ask-Me-Anything Prompting(@arora2022ama)

### A Watermark for Large Language Models(@kirchenbauer2023watermarking)
