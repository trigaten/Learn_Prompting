---
sidebar_position: 3
locale: cs-CZ
styl: chicago
---

# ğŸŸ¢ MyÅ¡lenkovÃ½ Å™etÄ›zec Prompting

Chain of Thought (CoT) prompting (@wei2022chain) je nedÃ¡vno vyvinutÃ½ prompting
metoda, kterÃ¡ vybÃ­zÃ­ Å¾Ã¡ka s LLM k vysvÄ›tlenÃ­ jeho uvaÅ¾ovÃ¡nÃ­. NÃ­Å¾e uvedenÃ½ obrÃ¡zek(@wei2022chain) 
ukazuje standardnÃ­ vÃ½zvu s %%nÄ›kolika snÃ­mky|standardnÃ­ vÃ½zvu s%nÄ›kolika snÃ­mky%% (vlevo) v porovnÃ¡nÃ­ s vÃ½zvou myÅ¡lenkovÃ©ho Å™etÄ›zce (vpravo).


import CoTExample from '@site/docs/assets/chain_of_thought_example.png';

<div style={{textAlign: 'center'}}>
  <img src={CoTExample} style={{width: "750px"}} />
</div>

<div style={{textAlign: 'center'}}>
lasickÃ½ prompting vs CoT (Wei et al.)
</div>

HlavnÃ­ myÅ¡lenka CoT spoÄÃ­vÃ¡ v tom, Å¾e ukÃ¡zÃ¡nÃ­m nÄ›kolika mÃ¡lo zÃ¡bÄ›rÅ¯ %%exemplÃ¡Å™Å¯|exemplÃ¡Å™Å¯%%, kde se argumentace
je proces uvaÅ¾ovÃ¡nÃ­ vysvÄ›tlen v exemplÃ¡Å™Ã­ch, LLM takÃ© ukÃ¡Å¾e proces uvaÅ¾ovÃ¡nÃ­ v exemplÃ¡Å™Ã­ch
pÅ™i odpovÃ­dÃ¡nÃ­ na vÃ½zvu. Toto vysvÄ›tlenÃ­ uvaÅ¾ovÃ¡nÃ­ Äasto vede k pÅ™esnÄ›jÅ¡Ã­mu
vÃ½sledkÅ¯m.

## PÅ™Ã­klad

Zde je nÄ›kolik ukÃ¡zek. PrvnÃ­ ukazuje GPT-3 (davinci-003)
kterÃ½ nedokÃ¡Å¾e vyÅ™eÅ¡it jednoduchou slovnÃ­ Ãºlohu. DruhÃ¡ ukazuje, jak GPT-3 (davinci-003) ÃºspÄ›Å¡nÄ› Å™eÅ¡Ã­ stejnou Ãºlohu pomocÃ­ vÃ½zvy CoT.

#### NesprÃ¡vnÄ›

<iframe
    src="https://embed.learnprompting.org/embed?config=eyJ0b3BQIjowLCJ0ZW1wZXJhdHVyZSI6MCwibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6Ik9wdGlvbiAxIGlzIGEgZmFzdGVyIHdheSB0byBnZXQgdG8gd29yay4iLCJwcm9tcHQiOiJXaGljaCBpcyBhIGZhc3RlciB3YXkgdG8gZ2V0IHRvIHdvcms%2FXG5PcHRpb24gMTogVGFrZSBhIDEwMDAgbWludXRlIGJ1cywgdGhlbiBhIGhhbGYgaG91ciB0cmFpbiwgYW5kIGZpbmFsbHkgYSAxMCBtaW51dGUgYmlrZSByaWRlLlxuT3B0aW9uIDI6IFRha2UgYW4gODAwIG1pbnV0ZSBidXMsIHRoZW4gYW4gaG91ciB0cmFpbiwgYW5kIGZpbmFsbHkgYSAzMCBtaW51dGUgYmlrZSByaWRlLiIsIm1vZGVsIjoidGV4dC1kYXZpbmNpLTAwMyJ9"
    style={{width:"100%", height:"500px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

#### SprÃ¡vnÄ›

<iframe
    src="https://embed.learnprompting.org/embed?config=eyJ0b3BQIjowLCJ0ZW1wZXJhdHVyZSI6MCwibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6Ik9wdGlvbiAxIHdpbGwgdGFrZSAxMDAwKzMwKzEwID0gMTA0MCBtaW51dGVzLlxuT3B0aW9uIDIgd2lsbCB0YWtlIDgwMCs2MCszMCA9IDg5MCBtaW51dGVzLlxuU2luY2UgT3B0aW9uIDIgdGFrZXMgODkwIG1pbnV0ZXMgYW5kIE9wdGlvbiAxIHRha2VzIDEwNDAgbWludXRlcywgT3B0aW9uIDIgaXMgZmFzdGVyLiIsInByb21wdCI6IldoaWNoIGlzIGEgZmFzdGVyIHdheSB0byBnZXQgaG9tZT9cbk9wdGlvbiAxOiBUYWtlIGFuIDEwIG1pbnV0ZXMgYnVzLCB0aGVuIGFuIDQwIG1pbnV0ZSBidXMsIGFuZCBmaW5hbGx5IGEgMTAgbWludXRlIHRyYWluLlxuT3B0aW9uIDI6IFRha2UgYSA5MCBtaW51dGVzIHRyYWluLCB0aGVuIGEgNDUgbWludXRlIGJpa2UgcmlkZSwgYW5kIGZpbmFsbHkgYSAxMCBtaW51dGUgYnVzLlxuT3B0aW9uIDEgd2lsbCB0YWtlIDEwKzQwKzEwID0gNjAgbWludXRlcy5cbk9wdGlvbiAyIHdpbGwgdGFrZSA5MCs0NSsxMD0xNDUgbWludXRlcy5cblNpbmNlIE9wdGlvbiAxIHRha2VzIDYwIG1pbnV0ZXMgYW5kIE9wdGlvbiAyIHRha2VzIDE0NSBtaW51dGVzLCBPcHRpb24gMSBpcyBmYXN0ZXIuXG5cbldoaWNoIGlzIGEgZmFzdGVyIHdheSB0byBnZXQgdG8gd29yaz9cbk9wdGlvbiAxOiBUYWtlIGEgMTAwMCBtaW51dGUgYnVzLCB0aGVuIGEgaGFsZiBob3VyIHRyYWluLCBhbmQgZmluYWxseSBhIDEwIG1pbnV0ZSBiaWtlIHJpZGUuXG5PcHRpb24gMjogVGFrZSBhbiA4MDAgbWludXRlIGJ1cywgdGhlbiBhbiBob3VyIHRyYWluLCBhbmQgZmluYWxseSBhIDMwIG1pbnV0ZSBiaWtlIHJpZGUuIiwibW9kZWwiOiJ0ZXh0LWRhdmluY2ktMDAzIn0%3D"
    style={{width:"100%", height:"500px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

## VÃ½sledky

UkÃ¡zalo se, Å¾e technologie CoT je ÃºÄinnÃ¡ pÅ™i zlepÅ¡ovÃ¡nÃ­ vÃ½sledkÅ¯ v ÃºlohÃ¡ch, jako je napÅ™. 
aritmetickÃ½ch ÃºlohÃ¡ch, ÃºlohÃ¡ch zdravÃ©ho rozumu a symbolickÃ©ho uvaÅ¾ovÃ¡nÃ­ (@wei2022chain). 
ZejmÃ©na pohotovÃ¡ PaLM 540B(@chowdhery2022palm) dosahuje 57 % Å™eÅ¡enÃ­ 
pÅ™esnosti v Ãºloze GSM8K(@cobbe2021training) (v tÃ© dobÄ› SOTA).

import PromptedPaLM from '@site/docs/assets/prompted_palm.png';

<div style={{textAlign: 'center'}}>
  <img src={PromptedPaLM} style={{width: "300px"}} />
</div>

<div style={{textAlign: 'center'}}>
Comparison of models on the GSM8K benchmark (Wei et al.)
</div>

## OmezenÃ­

DÅ¯leÅ¾itÃ© je, Å¾e podle Wei et al. "CoT pÅ™inÃ¡Å¡Ã­ zvÃ½Å¡enÃ­ vÃ½konu pouze pÅ™i pouÅ¾itÃ­ modelÅ¯ s parametry âˆ¼100B". MenÅ¡Ã­ modely zapisovaly nelogickÃ© myÅ¡lenkovÃ© Å™etÄ›zce, coÅ¾ vedlo k horÅ¡Ã­ pÅ™esnosti neÅ¾ standardnÃ­ napovÃ­dÃ¡nÃ­. Modely obvykle zÃ­skÃ¡vajÃ­ nÃ¡rÅ¯st vÃ½konu dÃ­ky podnÄ›tÅ¯m CoT zpÅ¯sobem ÃºmÄ›rnÃ½m velikosti modelu.


## PoznÃ¡mky

PÅ™i psanÃ­ tÃ©to kapitoly nebyly ~~poÅ¡kozeny~~ Å¾Ã¡dnÃ© jazykovÃ© modely, kterÃ© by byly finÃ¡lnÄ› vyladÄ›ny ğŸ˜Š.