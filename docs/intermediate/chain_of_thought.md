---
sidebar_position: 3
locale: cs-CZ
styl: chicago
---

# 游릭 Chain of Thought Prompting

Chain of Thought (CoT) prompting (@wei2022chain) je ned치vno vyvinut치 prompting metoda, kter치 vyb칤z칤 %%LLM|LLM%% k vysv캩tlen칤 jeho uva쬺v치n칤. N칤쬰 uveden칳 obr치zek(@wei2022chain) ukazuje %%standardn칤 prompt s n캩kolika shoty|few shot standard prompt%% (vlevo) v porovn치n칤 s %%promptem my코lenkov칠ho 콏et캩zce|Chain of Thought Prompting%% (vpravo).


import CoTExample from '@site/docs/assets/chain_of_thought_example.png';

<div style={{textAlign: 'center'}}>
  <img src={CoTExample} style={{width: "750px"}} />
</div>

<div style={{textAlign: 'center'}}>
klasick칳 prompting vs CoT (Wei et al.)
</div>

Hlavn칤 my코lenka %%CoT|Chain of Thought Prompting%% spo캜칤v치 v tom, 쬰 uk치z치n칤m n캩kolika m치lo z치b캩r콢 %%exempl치콏콢|exemplars%%, kde se argumentace vysv캩tluje v exempl치콏칤ch, LLM tak칠 uk치쬰 proces uva쬺v치n칤 p콏i odpov칤d치n칤 na prompt. Toto vysv캩tlen칤 uva쬺v치n칤 캜asto vede k p콏esn캩j코칤m v칳sledk콢m.

## P콏칤klad

Zde je n캩kolik uk치zek. Prvn칤 ukazuje jak GPT-3 (davinci-003) nedok치쬰 vy콏e코it jednoduchou slovn칤 칰lohu. Druh치 ukazuje, jak GPT-3 (davinci-003) 칰sp캩코n캩 콏e코칤 stejnou 칰lohu pomoc칤 v칳zvy CoT.

#### Nespr치vn캩

<iframe
    src="https://embed.learnprompting.org/embed?config=eyJ0b3BQIjowLCJ0ZW1wZXJhdHVyZSI6MCwibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6Ik9wdGlvbiAxIGlzIGEgZmFzdGVyIHdheSB0byBnZXQgdG8gd29yay4iLCJwcm9tcHQiOiJXaGljaCBpcyBhIGZhc3RlciB3YXkgdG8gZ2V0IHRvIHdvcms%2FXG5PcHRpb24gMTogVGFrZSBhIDEwMDAgbWludXRlIGJ1cywgdGhlbiBhIGhhbGYgaG91ciB0cmFpbiwgYW5kIGZpbmFsbHkgYSAxMCBtaW51dGUgYmlrZSByaWRlLlxuT3B0aW9uIDI6IFRha2UgYW4gODAwIG1pbnV0ZSBidXMsIHRoZW4gYW4gaG91ciB0cmFpbiwgYW5kIGZpbmFsbHkgYSAzMCBtaW51dGUgYmlrZSByaWRlLiIsIm1vZGVsIjoidGV4dC1kYXZpbmNpLTAwMyJ9"
    style={{width:"100%", height:"500px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

#### Spr치vn캩

<iframe
    src="https://embed.learnprompting.org/embed?config=eyJ0b3BQIjowLCJ0ZW1wZXJhdHVyZSI6MCwibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6Ik9wdGlvbiAxIHdpbGwgdGFrZSAxMDAwKzMwKzEwID0gMTA0MCBtaW51dGVzLlxuT3B0aW9uIDIgd2lsbCB0YWtlIDgwMCs2MCszMCA9IDg5MCBtaW51dGVzLlxuU2luY2UgT3B0aW9uIDIgdGFrZXMgODkwIG1pbnV0ZXMgYW5kIE9wdGlvbiAxIHRha2VzIDEwNDAgbWludXRlcywgT3B0aW9uIDIgaXMgZmFzdGVyLiIsInByb21wdCI6IldoaWNoIGlzIGEgZmFzdGVyIHdheSB0byBnZXQgaG9tZT9cbk9wdGlvbiAxOiBUYWtlIGFuIDEwIG1pbnV0ZXMgYnVzLCB0aGVuIGFuIDQwIG1pbnV0ZSBidXMsIGFuZCBmaW5hbGx5IGEgMTAgbWludXRlIHRyYWluLlxuT3B0aW9uIDI6IFRha2UgYSA5MCBtaW51dGVzIHRyYWluLCB0aGVuIGEgNDUgbWludXRlIGJpa2UgcmlkZSwgYW5kIGZpbmFsbHkgYSAxMCBtaW51dGUgYnVzLlxuT3B0aW9uIDEgd2lsbCB0YWtlIDEwKzQwKzEwID0gNjAgbWludXRlcy5cbk9wdGlvbiAyIHdpbGwgdGFrZSA5MCs0NSsxMD0xNDUgbWludXRlcy5cblNpbmNlIE9wdGlvbiAxIHRha2VzIDYwIG1pbnV0ZXMgYW5kIE9wdGlvbiAyIHRha2VzIDE0NSBtaW51dGVzLCBPcHRpb24gMSBpcyBmYXN0ZXIuXG5cbldoaWNoIGlzIGEgZmFzdGVyIHdheSB0byBnZXQgdG8gd29yaz9cbk9wdGlvbiAxOiBUYWtlIGEgMTAwMCBtaW51dGUgYnVzLCB0aGVuIGEgaGFsZiBob3VyIHRyYWluLCBhbmQgZmluYWxseSBhIDEwIG1pbnV0ZSBiaWtlIHJpZGUuXG5PcHRpb24gMjogVGFrZSBhbiA4MDAgbWludXRlIGJ1cywgdGhlbiBhbiBob3VyIHRyYWluLCBhbmQgZmluYWxseSBhIDMwIG1pbnV0ZSBiaWtlIHJpZGUuIiwibW9kZWwiOiJ0ZXh0LWRhdmluY2ktMDAzIn0%3D"
    style={{width:"100%", height:"500px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

## V칳sledky

Uk치zalo se, 쬰 technologie CoT je 칰캜inn치 p콏i zlep코ov치n칤 v칳sledk콢 v 칰loh치ch, jako je nap콏. aritmetick칳ch 칰loh치ch, 칰loh치ch zdrav칠ho rozumu a symbolick칠ho uva쬺v치n칤 (@wei2022chain). 
Zejm칠na pohotov치 PaLM 540B(@chowdhery2022palm) dosahuje 57% p콏esnosti v 콏e코en칤 칰lohy GSM8K(@cobbe2021training) (v t칠 dob캩 SOTA).

import PromptedPaLM from '@site/docs/assets/prompted_palm.png';

<div style={{textAlign: 'center'}}>
  <img src={PromptedPaLM} style={{width: "300px"}} />
</div>

<div style={{textAlign: 'center'}}>
Comparison of models on the GSM8K benchmark (Wei et al.)
</div>

## Omezen칤

D콢le쬴t칠 je, 쬰 podle Wei et al. _"CoT p콏in치코칤 zv칳코en칤 v칳konu pouze p콏i pou쬴t칤 model콢 s parametry cca 100B"_. Men코칤 modely zapisovaly nelogick칠 my코lenkov칠 콏et캩zce, co vedlo k hor코칤 p콏esnosti ne standardn칤 prompting. Modely obvykle z칤sk치vaj칤 n치r콢st v칳konu d칤ky podn캩t콢m CoT zp콢sobem 칰m캩rn칳m velikosti modelu.


## Pozn치mky

P콏i psan칤 t칠to kapitoly nebyly ~~po코kozeny~~ dolad캩ny 쮂멳n칠 jazykov칠 modely 游땕.