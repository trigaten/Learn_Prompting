---
sidebar_position: 1
---

# ğŸ”´ Soft prompty

Prompt tuning(@lester2021power), alternativa k fine tuningu modelu(@khashabi2021prompt), zmrazÃ­ vÃ¡hy modelu a aktualizuje parametry promptu. VÃ½slednÃ½ prompt je "mÄ›kkÃ½ prompt" (ang. soft prompt).


import Image from '../assets/prompt_tuning.png';

<div style={{textAlign: 'center'}}>
  <img src={Image} style={{width: "500px"}} />
</div>

<div style={{textAlign: 'center'}}>
Model Tuning vs Prompt Tuning (Lester et al.)
</div>

Na vÃ½Å¡e uvedenÃ©m obrÃ¡zku je kontrast mezi tuningem modelu a prompt tuningem. PÅ™i tuningu (ladÄ›nÃ­) modelu dolaÄujete stejnÃ½ model na rÅ¯znÃ½ch ÃºlohÃ¡ch. TÃ­m zÃ­skÃ¡te nÄ›kolik rÅ¯znÃ½ch modelÅ¯, s nimiÅ¾ nelze nutnÄ› snadno batchovat vstupy.

Naproti tomu ladÄ›nÃ­ promptu vÃ¡m umoÅ¾Åˆuje pouÅ¾Ã­t stejnÃ½ model pro vÅ¡echny Ãºlohy. StaÄÃ­ jen v dobÄ› inference doplnit sprÃ¡vnÃ© prompty, coÅ¾ usnadÅˆuje batching v rÅ¯znÃ½ch ÃºlohÃ¡ch. To je v podstatÄ› stejnÃ¡ vÃ½hoda, jakou mÃ¡ bÄ›Å¾nÃ½ prompting. NavÃ­c soft prompty natrÃ©novanÃ© pro jeden model ve vÃ­ce ÃºlohÃ¡ch budou mÃ­t Äasto stejnou dÃ©lku tokenu.

## Jak to funguje

Abychom pochopili zÃ¡kladnÃ­ logiku soft promptingu, zamysleme se nad tÃ­m, jak funguje **odvozovÃ¡nÃ­ modelu** na danÃ©m promptu: `Co je 2+2?`.

1) MÅ¯Å¾e bÃ½t tokenizovÃ¡na jako `Co, 's, 2, +, 2, ?`. 

2) Pak se kaÅ¾dÃ½ token pÅ™evede na vektor hodnot.

3) Tyto vektory hodnot lze povaÅ¾ovat za parametry modelu. Model mÅ¯Å¾e bÃ½t dÃ¡le trÃ©novaÃ¡n pouze Ãºpravou vah tÄ›chto promÄ›nnÃ½ch.

VÅ¡imnÄ›te si, Å¾e jakmile zaÄneme tyto vÃ¡hy aktualizovat, vektory tokenÅ¯ jiÅ¾ neodpovÃ­dajÃ­ skuteÄnÃ½m embeddingÅ¯m ze slovnÃ­ku.

# VÃ½sledky 

LadÄ›nÃ­ promptÅ¯ funguje lÃ©pe u vÄ›tÅ¡Ã­ch modelÅ¯. VÄ›tÅ¡Ã­ modely takÃ© vyÅ¾adujÃ­ mÃ©nÄ› soft prompt tokenÅ¯. Bez ohledu na to vÃ­ce neÅ¾ 20 tokenÅ¯ nepÅ™inÃ¡Å¡Ã­ vÃ½raznÃ½ nÃ¡rÅ¯st vÃ½konu.