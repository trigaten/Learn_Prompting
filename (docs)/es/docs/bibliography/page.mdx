export const metadata = { sidebar_position: 1000, title: "ðŸ“š Bibliography" };

# ðŸ“š Bibliography

The page contains an organized list of all papers used by this course.
The papers are organized by topic.

**To cite this course, use the provided citation in the Github repository.**

ðŸ”µ = Paper directly cited in this course. Other papers have informed my understanding of the topic.

Note: since [neither the GPT-3 nor the GPT-3 Instruct paper correspond to davinci models](https://twitter.com/janleike/status/1584618242756132864), I attempt not to
cite them as such.

## Prompt Engineering Strategies

#### Chain of Thought(@wei2022chain) ðŸ”µ

#### Zero Shot Chain of Thought(@kojima2022large) ðŸ”µ

#### Self Consistency(@wang2022selfconsistency) ðŸ”µ

#### What Makes Good In-Context Examples for GPT-3?(@liu2021makes) ðŸ”µ

#### Generated Knowledge(@liu2021generated) ðŸ”µ

#### Rethinking the role of demonstrations(@min2022rethinking) ðŸ”µ

#### Scratchpads(@nye2021work)

#### Maieutic Prompting(@jung2022maieutic)

#### STaR(@zelikman2022star)

#### Least to Most(@zhou2022leasttomost)

## Reliability

#### The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning(@ye2022unreliability) ðŸ”µ

#### Prompting GPT-3 to be reliable(@si2022prompting)

#### Diverse Prompts(@li2022advance) ðŸ”µ

#### Calibrate Before Use: Improving Few-Shot Performance of Language Models(@zhao2021calibrate) ðŸ”µ

#### Enhanced Self Consistency(@mitchell2022enhancing)

#### Bias and Toxicity in Zero-Shot CoT(@shaikh2022second) ðŸ”µ

#### Constitutional AI: Harmlessness from AI Feedback (@bai2022constitutional) ðŸ”µ

## Automated Prompt Engineering

#### AutoPrompt(@shin2020autoprompt) ðŸ”µ

#### Automatic Prompt Engineer(@zhou2022large)

## Models

### Language Models

#### GPT-3(@brown2020language) ðŸ”µ

#### GPT-3 Instruct(@ouyang2022training) ðŸ”µ

#### PaLM(@chowdhery2022palm) ðŸ”µ

#### BLOOM(@scao2022bloom) ðŸ”µ

#### BLOOM+1 (more languages/ 0 shot improvements)(@yong2022bloom1)

#### Jurassic 1(@lieberjurassic) ðŸ”µ

#### GPT-J-6B(@wange2021gptj)

#### Roberta(@liu2019roberta)

### Image Models

#### Stable Diffusion(@rombach2021highresolution) ðŸ”µ

#### DALLE(@ramesh2022hierarchical) ðŸ”µ

## Soft Prompting

#### Soft Prompting(@lester2021power) ðŸ”µ

#### Interpretable Discretized Soft Prompts(@khashabi2021prompt) ðŸ”µ

## Datasets

#### GSM8K(@cobbe2021training) ðŸ”µ

#### HotPotQA(@yang2018hotpotqa) ðŸ”µ

#### Fever(@thorne2018fever) ðŸ”µ

#### BBQ: A Hand-Built Bias Benchmark for Question Answering(@parrish2021bbq) ðŸ”µ

## Image Prompt Engineering

#### Taxonomy of prompt modifiers(@oppenlaender2022taxonomy)

#### DiffusionDB(@wang2022diffusiondb)

#### The DALLE 2 Prompt Book(@parsons2022dalleprompt) ðŸ”µ

#### Prompt Engineering for Text-Based Generative Art(@oppenlaender2022prompt) ðŸ”µ

#### With the right prompt, Stable Diffusion 2.0 can do hands.(@blake2022with) ðŸ”µ

#### Optimizing Prompts for Text-to-Image Generation(@hao2022optimizing)

## Prompt Engineering IDEs

#### Prompt IDE(@strobelt2022promptide) ðŸ”µ

#### Prompt Source(@bach2022promptsource) ðŸ”µ

#### PromptChainer(@wu2022promptchainer) ðŸ”µ

#### PromptMaker(@jiang2022promptmaker) ðŸ”µ

## Tooling

#### LangChain(@Chase_LangChain_2022) ðŸ”µ

#### TextBox 2.0: A Text Generation Library with Pre-trained Language Models(@tang2022textbox) ðŸ”µ

#### OpenPrompt: An Open-source Framework for Prompt-learning(@ding2021openprompt) ðŸ”µ

#### GPT Index(@Liu_GPT_Index_2022) ðŸ”µ

## Applied Prompt Engineering

#### Language Model Cascades(@dohan2022language)

#### MRKL(@karpas2022mrkl) ðŸ”µ

#### ReAct(@yao2022react) ðŸ”µ

#### PAL: Program-aided Language Models(@gao2022pal) ðŸ”µ

## User Interface Design

#### Design Guidelines for Prompt Engineering Text-to-Image Generative Models(@liu2022design)

## Prompt Injection

#### Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods(@crothers2022machine) ðŸ”µ

#### Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples(@branch2022evaluating) ðŸ”µ

#### Prompt injection attacks against GPT-3(@simon2022inject) ðŸ”µ

#### Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions(@goodside2022inject) ðŸ”µ

#### adversarial-prompts(@chase2021adversarial) ðŸ”µ

#### GPT-3 Prompt Injection Defenses(@goodside2021gpt) ðŸ”µ

#### Talking to machines: prompt engineering & injection(@christoph2022talking)

#### Exploring Prompt Injection Attacks(@selvi2022exploring) ðŸ”µ

#### Using GPT-Eliezer against ChatGPT Jailbreaking(@armstrong2022using) ðŸ”µ

## Jailbreaking

#### Ignore Previous Prompt: Attack Techniques For Language Models(@perez2022jailbreak)

#### Lessons learned on Language Model Safety and misuse(@brundage_2022)

#### Toxicity Detection with Generative Prompt-based Inference(@wang2022jailbreak)

#### New and improved content moderation tooling(@markov_2022)

#### OpenAI API(@openai_api) ðŸ”µ

#### OpenAI ChatGPT(@openai_chatgpt) ðŸ”µ

#### ChatGPT 4 Tweet(@alice2022jailbreak) ðŸ”µ

#### Acting Tweet(@miguel2022jailbreak) ðŸ”µ

#### Research Tweet(@derek2022jailbreak) ðŸ”µ

#### Pretend Ability Tweet(@nero2022jailbreak) ðŸ”µ

#### Responsibility Tweet(@nick2022jailbreak) ðŸ”µ

#### Lynx Mode Tweet(@jonas2022jailbreak) ðŸ”µ

#### Sudo Mode Tweet(@sudo2022jailbreak) ðŸ”µ

#### Ignore Previous Prompt(@ignore_previous_prompt) ðŸ”µ

## Surveys

#### Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing(@liu2021pretrain)

#### PromptPapers(@ning2022papers)

## Dataset Generation

#### Discovering Language Model Behaviors with Model-Written Evaluations(@perez2022discovering)

#### Selective Annotation Makes Language Models Better Few-Shot Learners(@su2022selective)

## Applications

#### Atlas: Few-shot Learning with Retrieval Augmented Language Models(@izacard2022atlas)

#### STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension(@wang2022strudel)

## Miscl

#### Prompting Is Programming: A Query Language For Large Language Models(@beurerkellner2022prompting)

#### Parallel Context Windows Improve In-Context Learning of Large Language Models(@ratner2022parallel)

#### Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models(@bursztyn2022learning)

#### Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks(@wang2022supernaturalinstructions)

#### Making Pre-trained Language Models Better Few-shot Learners(@gao2021making)

#### Grounding with search results(@livin2022large)

#### How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models(@dang2022prompt)

#### On Measuring Social Biases in Prompt-Based Multi-Task Learning(@akyrek2022measuring)

#### Plot Writing From Pre-Trained Language Models(@jin2022plot) ðŸ”µ

#### StereoSet: Measuring stereotypical bias in pretrained language models(@nadeem-etal-2021-stereoset)

#### Survey of Hallucination in Natural Language Generation(@Ji_2022)

#### Examples(@liu2021makes)

#### Wordcraft(@yuan2022wordcraft)

#### PainPoints(@fadnavis2022pain)

#### Self-Instruct: Aligning Language Model with Self Generated Instructions(@wang2022selfinstruct)

#### From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models(@guo2022images)

#### Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference(@schick2020exploiting)

### A Watermark for Large Language Models(@kirchenbauer2023watermarking)
