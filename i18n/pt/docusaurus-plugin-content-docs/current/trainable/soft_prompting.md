---
sidebar_position: 1
---

# üî¥ Prompts Suaves

A afina√ß√£o de prompts (@lester2021power), uma alternativa √† afina√ß√£o fina de modelos (@khashabi2021prompt), congela os pesos do modelo e atualiza os par√¢metros de um prompt. O prompt resultante √© um 'prompt suave'.


import Image from '@site/docs/assets/trainable/prompt_tuning.webp';

<div style={{textAlign: 'center'}}>
  <img src={Image} style={{width: "500px"}}/>
</div>

<div style={{textAlign: 'center'}}>
Ajuste de Modelo vs Ajuste de Prompt (Lester et al.)
</div>

A imagem acima contrasta o ajuste de modelo com o ajuste de prompt. No ajuste de modelo, voc√™ afina o mesmo modelo em diferentes tarefas. Isso lhe d√° alguns modelos diferentes, com os quais n√£o √© necessariamente f√°cil agrupar entradas.

Por outro lado, o ajuste de prompt permite que voc√™ use o mesmo modelo para todas as tarefas. Voc√™ s√≥ precisa anexar os prompts apropriados no momento da infer√™ncia, o que facilita o agrupamento entre tarefas diferentes. Isso √© praticamente a mesma vantagem que o prompting regular tem. Al√©m disso, prompts suaves treinados para um √∫nico modelo em v√°rias tarefas muitas vezes ter√£o o mesmo comprimento de token.

## Como funciona

Para entender a l√≥gica b√°sica por tr√°s do prompt suave, pense em como a **infer√™ncia do modelo** funciona em um prompt espec√≠fico: `"Quanto √© 2+2?"`.

1) Pode ser tokenizado como What, 's, 2, +, 2, ?.

2) Em seguida, cada token ser√° convertido em um vetor de valores.

3) Esses vetores de valores podem ser considerados como par√¢metros do modelo. O modelo pode ser posteriormente treinado, ajustando apenas os pesos desses prompts.

Observe que assim que come√ßamos a atualizar esses pesos, os vetores dos tokens n√£o correspondem mais a embeddings reais do vocabul√°rio.

# Resultados

O ajuste de prompt tem melhor desempenho com modelos maiores. Modelos maiores tamb√©m requerem menos tokens de prompt suave. Independentemente disso, mais de 20 tokens n√£o proporcionam ganhos significativos de desempenho.