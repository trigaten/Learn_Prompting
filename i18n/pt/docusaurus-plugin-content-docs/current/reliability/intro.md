---
sidebar_position: 1
---

# üü¢ Introdu√ß√£o

Este cap√≠tulo aborda como tornar os resultados de prompt podem ser mais confi√°veis e como implementar verifica√ß√µes para garantir que essas sa√≠das sejam confi√°veis.

At√© certo ponto, a maioria dos t√©cnicas abordadas anteriormente t√™m a ver com a melhoria da precis√£o da sa√≠da e, portanto, da confiabilidade, especialmente da autoconsist√™ncia (@wang2022selfconsistency). No entanto, existem outras t√©cnicas que podem ser usadas para melhorar a confiabilidade, al√©m de estrat√©gias de engenharia de prompt b√°sicas.

Os modelos de linguagem comuns (%%LLMs|LLM%%) foram considerados mais confi√°veis do que esper√°vamos na interpreta√ß√£o do que uma prompt est√° tentando dizer, mesmo quando ela cont√©m erros de ortografia, frases mal formuladas ou at√© mesmo informa√ß√µes enganosas (@webson2023itscomplicated). Apesar dessa capacidade, ainda apresentam v√°rios problemas, incluindo alucina√ß√µes (@ye2022unreliability), explica√ß√µes falhas com m√©todos de prompt Cadeia de Pensamento (CdP) (@ye2022unreliability) e vieses m√∫ltiplos, incluindo vi√©s de r√≥tulo majorit√°rio, vi√©s de recenticidade e vi√©s de token comum (@zhao2021calibrate). Al√©m disso, a t√©cnica de CdP sem o uso de amostras pode ser particularmente tendenciosa ao lidar com t√≥picos sens√≠veis (@shaikh2022second).

Solu√ß√µes comuns para alguns desses problemas incluem calibradores para remover os vi√©s _a priori_, verificadores para avaliar as conclus√µes, bem como promover a diversidade nas conclus√µes.

%%LLMs|LLM%% have been found to be more reliable than we might expect at interpreting what a prompt is *trying* to say when responding to misspelled, badly phrased, or even actively misleading prompts(@webson2023itscomplicated). 
Despite this ability, they still exhibit various problems including hallucinations(@ye2022unreliability), 
flawed explanations with %%CoT|CoT prompting%% methods(@ye2022unreliability), and multiple biases
including majority label bias, recency bias, and common token bias(@zhao2021calibrate). 
Additionally, zero-shot CoT can be particularly biased when dealing with sensitive topics
(@shaikh2022second).

Common solutions to some of these problems include calibrators to remove _a priori_ biases,
and verifiers to score completions, as well as promoting diversity in completions.