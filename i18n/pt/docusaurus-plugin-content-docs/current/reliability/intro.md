---
sidebar_position: 1
---

# üü¢ Introdu√ß√£o

Este cap√≠tulo aborda como tornar os resultados de prompt podem ser mais confi√°veis e como implementar verifica√ß√µes para garantir que essas sa√≠das sejam confi√°veis.

At√© certo ponto, a maioria dos t√©cnicas abordadas anteriormente t√™m a ver com a melhoria da precis√£o da sa√≠da e, portanto, da confiabilidade, especialmente da autoconsist√™ncia (@wang2022selfconsistency). No entanto, existem outras t√©cnicas que podem ser usadas para melhorar a confiabilidade, al√©m de estrat√©gias de engenharia de prompt b√°sicas.

Os modelos de linguagem comuns (%%LLMs|LLM%%) foram considerados mais confi√°veis do que esper√°vamos na interpreta√ß√£o do que uma prompt est√° tentando dizer, mesmo quando ela cont√©m erros de ortografia, frases mal formuladas ou at√© mesmo informa√ß√µes enganosas (@webson2023itscomplicated). Apesar dessa capacidade, ainda apresentam v√°rios problemas, incluindo alucina√ß√µes (@ye2022unreliability), explica√ß√µes falhas com m√©todos de prompt Cadeia de Pensamento (CdP) (@ye2022unreliability) e vieses m√∫ltiplos, incluindo vi√©s de r√≥tulo majorit√°rio, vi√©s de recenticidade e vi√©s de token comum (@zhao2021calibrate). Al√©m disso, a t√©cnica de CdP sem o uso de amostras pode ser particularmente tendenciosa ao lidar com t√≥picos sens√≠veis (@shaikh2022second).

Solu√ß√µes comuns para alguns desses problemas incluem calibradores para remover os vi√©s _a priori_, verificadores para avaliar as conclus√µes, bem como promover a diversidade nas conclus√µes.