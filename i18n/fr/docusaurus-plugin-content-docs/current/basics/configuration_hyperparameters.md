---
sidebar_position: 100
---

# üü¢ Param√®tres LLM

import Temperature from '@site/docs/assets/basics/temperature.svg';

<div style={{textAlign: 'center'}}>
  <Temperature style={{width:"500px",height:"300px",verticalAlign:"top"}}/>
</div>

# Introduction

La sortie des LLM peut √™tre affect√©e par des _hyperparam√®tres de configuration_, qui contr√¥lent de divers aspects du mod√®le, tels que son caract√®re "al√©atoire". Ces hyperparam√®tres peuvent √™tre ajust√©s pour produire une sortie plus cr√©ative, diverse et int√©ressante. Dans cette section, nous discuterons de deux hyperparam√®tres de configuration importants et de leur impact sur la sortie des LLM.

:::note
[pour les chercheurs] Ceux-ci sont diff√©rents des hyperparam√®tres r√©guliers tels que le taux d'apprentissage, le nombre de couches, la taille cach√©e, etc.
:::

## Temp√©rature

La temp√©rature est un hyperparam√®tre de configuration qui contr√¥le le caract√®re al√©atoire de la sortie du mod√®le de langage. Une temp√©rature √©lev√©e produit des r√©sultats plus impr√©visibles et cr√©atifs, tandis qu'une temp√©rature basse produit une sortie plus commune et conservatrice. Par exemple, si vous r√©glez la temp√©rature sur 0,5, le mod√®le g√©n√©rera g√©n√©ralement un texte plus pr√©visible et moins cr√©atif que si vous r√©glez la temp√©rature sur 1,0.

## Top p

Top p, √©galement connu sous le nom de nucleus sampling (√©chantillonnage de noyau), est un autre hyperparam√®tre de configuration qui contr√¥le le caract√®re al√©atoire de la sortie du mod√®le de langage. Il fixe une probabilit√© seuil et s√©lectionne les meilleurs tokens dont la probabilit√© cumulative d√©passe le seuil. Le mod√®le √©chantillonne ensuite au hasard dans cet ensemble de tokens pour g√©n√©rer une sortie. Cette m√©thode peut produire une sortie plus diversifi√©e et int√©ressante que les m√©thodes traditionnelles qui √©chantillonnent au hasard tout le vocabulaire. Par exemple, si vous fixez le top p √† 0,9, le mod√®le ne consid√©rera que les mots les plus probables qui repr√©sentent 90 % de la masse de probabilit√©.

## Autres hyperparam√®tres pertinents

Il existe de nombreux autres hyperparam√®tres qui peuvent affecter les performances du mod√®le de langage, tels que les p√©nalit√©s de fr√©quence et de pr√©sence. Nous ne les couvrons pas ici, mais peut-√™tre le ferons-nous √† l'avenir.

## Comment ces hyperparam√®tres affectent la sortie

La temp√©rature et le top p peuvent tous deux affecter la sortie d'un mod√®le de langage en contr√¥lant le degr√© d'al√©atoire et de diversit√© dans le texte g√©n√©r√©. Une valeur √©lev√©e de temp√©rature ou de top p produit des r√©sultats plus impr√©visibles et int√©ressants, mais augmente √©galement la probabilit√© d'erreurs ou de texte absurde. Une valeur basse de temp√©rature ou de top p peut produire des r√©sultats plus conservateurs et pr√©visibles, mais peut √©galement entra√Æner un texte r√©p√©titif ou peu int√©ressant.

Pour les t√¢ches de g√©n√©ration de texte, vous voudrez peut-√™tre utiliser une valeur √©lev√©e de temp√©rature ou de top p. Cependant, pour les t√¢ches o√π l'exactitude est importante, telles que les t√¢ches de traduction ou de r√©ponse aux questions, une valeur basse de temp√©rature ou de top p doit √™tre utilis√©e pour am√©liorer l'exactitude et l'exactitude factuelle.

:::note
Parfois, plus d'al√©atoire peut √™tre utile pour les t√¢ches o√π l'exactitude est n√©cessaire lorsqu'il est associ√© √† [des techniques de prompting sp√©ciales](https://learnprompting.org/docs/intermediate/self_consistency).
:::

## Conclusion

En r√©sum√©, la temp√©rature, le top p et les autres hyperparam√®tres de configuration du mod√®le sont des facteurs cl√©s √† prendre en compte lors de l'utilisation de mod√®les de langage. En comprenant la relation entre ces hyperparam√®tres et la sortie du mod√®le, les praticiens peuvent optimiser leurs prompts pour des t√¢ches et des applications sp√©cifiques.

:::warning
Certains mod√®les, comme ChatGPT, ne vous permettent **pas** d'ajuster ces hyperparam√®tres de configuration.
:::

Par jackdickens382
