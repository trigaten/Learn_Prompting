---
sidebar_position: 100
---

# 游릭 Ajustes del LLM

import Temperature from '@site/docs/assets/basics/temperature.svg';

<div style={{textAlign: 'center'}}>
  <Temperature style={{width:"500px",height:"300px",verticalAlign:"top"}}/>
</div>

# Introducci칩n

Los resultados de los LLM pueden verse afectados por los _hiperpar치metros de configuraci칩n_, que controlan varios aspectos del modelo, como su grado de "aleatoriedad". Estos hiperpar치metros pueden ajustarse para producir resultados m치s creativos, diversos e interesantes. En esta secci칩n, discutiremos dos hiperpar치metros de configuraci칩n importantes y c칩mo afectan a los resultados de los LLM.

:::note
[para investigadores] Son diferentes de los hiperpar치metros normales, como la tasa de aprendizaje, el n칰mero de capas, el tama침o oculto, etc.
:::

## Temperatura

La temperatura es un hiperpar치metro de configuraci칩n que controla la aleatoriedad de los resultados del modelo ling칲칤stico. Una temperatura alta produce resultados m치s impredecibles y creativos, mientras que una temperatura baja produce resultados m치s comunes y conservadores. Por ejemplo, si ajusta la temperatura a 0.5, el modelo generar치 normalmente un texto m치s predecible y menos creativo que si ajusta la temperatura a 1.0.

## Top p

Top p, tambi칠n conocido como muestreo de n칰cleos, es otro hiperpar치metro de configuraci칩n que controla la aleatoriedad de la salida del modelo ling칲칤stico. Establece un umbral de probabilidad y selecciona los tokens superiores cuya probabilidad acumulada supera el umbral. A continuaci칩n, el modelo toma muestras aleatorias de este conjunto de tokens para generar la salida. Este m칠todo puede producir resultados m치s diversos e interesantes que los m칠todos tradicionales, que muestrean aleatoriamente todo el vocabulario. Por ejemplo, si se fija top p en 0,9, el modelo s칩lo tendr치 en cuenta las palabras m치s probables que constituyan el 90% de la masa de probabilidad.

## Otros hiperpar치metros relevantes

Hay muchos otros hiperpar치metros que pueden afectar el rendimiento del modelo de lenguaje, como la frecuencia y las penalizaciones por presencia. No los cubrimos aqu칤, pero tal vez lo haremos en el futuro.

## C칩mo estos hiperpar치metros afectan la salida

Tanto la temperatura como el valor top p pueden afectar al resultado de un modelo ling칲칤stico controlando el grado de aleatoriedad y diversidad del texto generado. Un valor alto de temperatura o de top p produce resultados m치s impredecibles e interesantes, pero tambi칠n aumenta la probabilidad de errores o de texto sin sentido. Un valor bajo de temperatura o de Top p pueden producir resultados m치s conservadores y predecibles, pero tambi칠n pueden dar lugar a texto repetitivo o poco interesante.

Para tareas de generaci칩n de texto, puede que le interese utilizar una temperatura alta o un valor p alto. Sin embargo, para las tareas en las que la precisi칩n es importante, como las tareas de traducci칩n o la respuesta a preguntas, se debe utilizar una temperatura baja o un valor p superior para mejorar la precisi칩n y la correcci칩n factual.

:::note
A veces, m치s aleatoriedad puede ser 칰til en tareas donde la precisi칩n es necesaria cuando se combina con [t칠cnicas especiales de prompting](https://learnprompting.org/docs/intermediate/self_consistency).
:::

## Conclusi칩n

En resumen, la temperatura, el top p y otros hiperpar치metros de configuraci칩n del modelo son factores clave a tener en cuenta cuando se trabaja con modelos ling칲칤sticos. Al comprender la relaci칩n entre estos hiperpar치metros y el resultado del modelo, los profesionales pueden optimizar sus prompts para tareas y aplicaciones espec칤ficas.

:::warning
Algunos modelos, como ChatGPT, **no** permiten ajustar estos hiperpar치metros de configuraci칩n.
:::

Por jackdickens382
