---
sidebar_position: 3
locale: es-mx
style: chicago
---

# 游릭 Cadena de pensamiento

El prompting Chain of Thought (CoT) (@wei2022chain) es un m칠todo de prompt recientemente desarrollado que alienta al LLM a explicar su razonamiento. La siguiente imagen (@wei2022chain) muestra un %%few shot standard prompt|prompt est치ndar de pocos ejemplos%% (izquierda) en comparaci칩n con un prompt de Cadena de Pensamiento (derecha).

import CoTExample from '@site/docs/assets/basics/chain_of_thought_example.webp';

<div style={{textAlign: 'center'}}>
  <img src={CoTExample} style={{width: "750px"}}/>
</div>

<div style={{textAlign: 'center'}}>
Prompting regular vs. CoT (Wei et al.)
</div>

La idea principal de CoT es que al mostrarle al LLM algunos %%ejemplos|ejemplos%% de few shots donde se explica el proceso de razonamiento, el LLM tambi칠n mostrar치 el proceso de razonamiento al responder al prompt. Esta explicaci칩n del razonamiento a menudo conduce a resultados m치s precisos.

## Ejemplo

Aqu칤 hay algunas demostraciones. La primera muestra a GPT-3 (davinci-003) fallando al resolver un problema de palabras simple. La segunda muestra a GPT-3 (davinci-003) resolviendo con 칠xito el mismo problema, utilizando un prompt de CoT.

#### Incorrecto

<div trydyno-embed="" openai-model="text-davinci-003" initial-prompt="쮺u치l es la forma m치s r치pida de llegar al trabajo?\nOpci칩n 1: Tomar un autob칰s de 1000 minutos, luego un tren de media hora y finalmente un paseo en bicicleta de 10 minutos.\nOpci칩n 2: Tomar un autob칰s de 800 minutos, luego un tren de una hora y finalmente un paseo en bicicleta de 30 minutos." initial-response="La opci칩n 1 es una forma m치s r치pida de llegar al trabajo." max-tokens="256" box-rows="7" model-temp="0" top-p="0"></div>

#### Correcto

<div trydyno-embed="" openai-model="text-davinci-003" initial-prompt="쮺u치l es la forma m치s r치pida de llegar a casa?\nOpci칩n 1: Tomar un autob칰s de 10 minutos, luego un autob칰s de 40 minutos y finalmente un tren de 10 minutos.\nOpci칩n 2: Tomar un tren de 90 minutos, luego un paseo en bicicleta de 45 minutos y finalmente un autob칰s de 10 minutos.\nOpci칩n 1: tomar치 10+40+10=60 minutos.\nOpci칩n 2: tomar치 90+45+10=145 minutos.\nSince Dado que la opci칩n 1 tarda 60 minutos y la opci칩n 2 tarda 145 minutos, la opci칩n 1 es m치s r치pida.\n\n쮺u치l es la forma m치s r치pida de llegar al trabajo? \nOpci칩n 1: Tomar un autob칰s de 1000 minutos, luego un tren de media hora y finalmente un paseo en bicicleta de 10 minutos. \nOpci칩n 2: Tomar un autob칰s de 800 minutos, luego un tren de una hora y finalmente un paseo en bicicleta de 30 minutos." initial-response="La opci칩n 1 tomar치 1000+30+10=1040 minutos.
La opci칩n 2 tomar치 800+60+30=890 minutos.
Dado que la opci칩n 2 tarda 890 minutos y la opci칩n 1 tarda 1040 minutos, la opci칩n 2 es m치s r치pida." max-tokens="256" box-rows="18" model-temp="0" top-p="0"></div>

## Resultados

Se ha demostrado que CoT es efectivo para mejorar los resultados en tareas como aritm칠tica, sentido com칰n y tareas de razonamiento simb칩lico (@wei2022chain). En particular, PaLM 540B con prompting (@chowdhery2022palm) logra una precisi칩n de tasa de resoluci칩n del 57% en GSM8K (@cobbe2021training) (SOTA en ese momento).

import PromptedPaLM from '@site/docs/assets/intermediate/prompted_palm.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={PromptedPaLM} style={{width: "300px"}} />
</div>

<div style={{textAlign: 'center'}}>
Comparaci칩n de modelos en la prueba de referencia GSM8K (Wei et al.)
</div>

## Limitaciones

Es importante destacar que, seg칰n Wei et al., "CoT solo produce mejoras de rendimiento cuando se usa con modelos de alrededor de 100 mil millones de par치metros". Los modelos m치s peque침os escribieron cadenas de pensamiento il칩gicas, lo que condujo a una precisi칩n peor que la del prompting est치ndar. Los modelos suelen obtener mejoras de rendimiento del prompting CoT de manera proporcional al tama침o del modelo.

## Notas

Ning칰n modelo de lenguaje fue ~~da침ado~~ ajustado en el proceso de escribir este cap칤tulo 游땕.
