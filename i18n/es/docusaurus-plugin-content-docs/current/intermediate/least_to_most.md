---
sidebar_position: 7
locale: es-mx
style: chicago
---

# 游리 Least to Most Prompting

Prompting de menos a m치s (LtM) (Zhou et al., 2022) lleva la t칠cnica de %%CoT prompting|prompting de CoT%% un paso m치s all치 al descomponer un problema en subproblemas y resolver cada uno. Esta t칠cnica est치 inspirada en estrategias educativas del mundo real para ni침os.

Al igual que en CoT prompting, el problema a resolver se descompone en un conjunto de subproblemas que se construyen uno sobre otro. En una segunda etapa, estos subproblemas se resuelven uno por uno. A diferencia de la cadena de pensamiento, la soluci칩n de los subproblemas anteriores se alimenta en el prompt para tratar de resolver el siguiente problema.

import leastToMost from '@site/docs/assets/intermediate/least_to_most_formal.webp'

<div style={{textAlign: 'center'}}>
  <img src={leastToMost} style={{width: "600px"}} alt="A diagram of a least to most prompting"/>
</div>

<div style={{textAlign: 'center'}}>
   Diagram of a Least to Most prompting
</div>

## Ejemplo: Respuesta a una consulta del cliente

Formulemos una pregunta de atenci칩n al cliente un poco complicada:

<iframe
    src="http://embed.learnprompting.org/embed?config=eyJib3hSb3dzIjoyNSwidG9wUCI6MSwidGVtcGVyYXR1cmUiOjAuNywibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6IiIsInByb21wdCI6IiIsIm1vZGVsIjoiZ3B0LTQiLCJ1bmRlZmluZWQiOiIwIn0%3D"
    style={{width:"100%", height:"1250px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>
<br/>Esto ha fallado (estamos dentro del tiempo de retorno), as칤 que vamos a intentar dividirlo en subproblemas:

<iframe
    src="http://embed.learnprompting.org/embed?config=eyJib3hSb3dzIjoyNSwidG9wUCI6MSwidGVtcGVyYXR1cmUiOjAuNywibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6IiIsInByb21wdCI6IiIsIm1vZGVsIjoiZ3B0LTQiLCJ1bmRlZmluZWQiOiIwIn0%3D"
    style={{width:"100%", height:"1250px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

<br/>Tratemos de resolver el primer subproblema:

<iframe
    src="http://embed.learnprompting.org/embed?config=eyJib3hSb3dzIjoyNSwidG9wUCI6MSwidGVtcGVyYXR1cmUiOjAuNywibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6IiIsInByb21wdCI6IiIsIm1vZGVsIjoiZ3B0LTQiLCJ1bmRlZmluZWQiOiIwIn0%3D"
    style={{width:"100%", height:"1250px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

Con s칩lo resolver el primer subproblema, pod칤amos resolver todo el problema. Si GPT-3 no devolv칤a una respuesta inmediatamente, podr칤amos haber resuelto el siguiente subproblema y as칤 sucesivamente hasta que devolviera una respuesta. Obs칠rvese que utilizamos `Vayamos paso a paso`. La adici칩n de esta frase no siempre es necesaria, pero ayuda para este ejemplo.

## Ejemplo: concatenaci칩n de letras

LtM se introdujo originalmente utilizando una solicitud de pocos ejemplos, en lugar de una instrucci칩n expl칤cita para descomponer un problema en m칰ltiples pasos (como se ve arriba). Adem치s, a veces se puede implementar con una sola solicitud en lugar de solicitudes concatenadas. Examinemos el problema de concatenar la 칰ltima letra de palabras individuales (@wei2022chain) (por ejemplo, dadas las palabras de entrada cake, etymology, la salida deber칤a ser ey).

### Primer intento: Est치ndar

El prompt est치ndar con ejemplos de few-shot funciona muy mal, incluso con un modelo m치s avanzado como text-davinci-003.

<iframe
    src="http://embed.learnprompting.org/embed?config=eyJib3hSb3dzIjoyNSwidG9wUCI6MSwidGVtcGVyYXR1cmUiOjAuNywibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6IiIsInByb21wdCI6IiIsIm1vZGVsIjoiZ3B0LTQiLCJ1bmRlZmluZWQiOiIwLjIifQ%3D%3D"
    style={{width:"100%", height:"1250px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

### Segundo intento: Cadena de pensamiento (CoT)

Chain of Thought obtiene unos resultados significativamente mejores que los de la incitaci칩n est치ndar. Esto se debe a que ahora permite al modelo considerar la extracci칩n de la 칰ltima letra de cada palabra por s칤 mismo, reduciendo la complejidad a la operaci칩n de agrupar letras que ha recogido previamente. Sin embargo, esto empieza a fallar con tama침os m치s grandes.

<iframe
    src="http://embed.learnprompting.org/embed?config=eyJib3hSb3dzIjoyNSwidG9wUCI6MSwidGVtcGVyYXR1cmUiOjAuNywibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6IiIsInByb21wdCI6IiIsIm1vZGVsIjoiZ3B0LTQiLCJ1bmRlZmluZWQiOiIwLjIifQ%3D%3D"
    style={{width:"100%", height:"1250px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

### Tercer intento: De menor a mayor (un solo prompt)

Con el m칠todo de menor a mayor, aumentamos el concepto de cadena de pensamiento reformulando los pasos individuales para volver a expresar el resultado concatenado previamente. De este modo, cada paso se reduce a concatenar una sola letra nueva. As칤 se obtienen buenos resultados hasta 12 o m치s palabras.

Este enfoque puede parecer muy similar al de la Cadena de Pensamiento, pero es conceptualmente muy diferente. Aqu칤, en cada paso, introducimos la concatenaci칩n anterior. En el caso de "think, machine, learning", en lugar de concatenar las letras "k", "e", "g" individualmente, concatenar치 "k" y "e", luego "ke" y "g". Como resultado de esta reintroducci칩n del trabajo anterior, el modelo puede ahora generalizarse a cadenas mucho m치s largas porque lleva el resultado de forma incremental y s칩lo necesita hacer una peque침a cantidad de trabajo en cada paso.

<iframe
    src="http://embed.learnprompting.org/embed?config=eyJib3hSb3dzIjoyNSwidG9wUCI6MSwidGVtcGVyYXR1cmUiOjAuNywibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6IiIsInByb21wdCI6IiIsIm1vZGVsIjoiZ3B0LTQiLCJ1bmRlZmluZWQiOiIwLjIifQ%3D%3D"
    style={{width:"100%", height:"1250px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

### Resultados

En el 칰ltimo problema de concatenaci칩n de letras con 12 palabras, Chain of Thought tiene una precisi칩n del 34%, mientras que Least to Most tiene una precisi칩n del 74% (el art칤culo utiliza text-davinci-002 como modelo).

## Ejemplo: generalizaci칩n composicional (SCAN)

La prueba de referencia SCAN (@lake2018scan) requiere que el modelo convierta el lenguaje natural en secuencias de acciones. Por ejemplo, la frase "correr a la izquierda y caminar dos veces" se traducir칤a como " TURN_LEFT + RUN + WALK \* 2". Los modelos ling칲칤sticos funcionan especialmente mal cuando se enfrentan a secuencias m치s largas que las del conjunto de entrenamiento.

### Primer intento: Standard prompting

Utilizando prompts est치ndar simples, text-davinci-003 llega impresionantemente lejos, pero sigue fallando.

<iframe
    src="http://embed.learnprompting.org/embed?config=eyJib3hSb3dzIjoyNSwidG9wUCI6MSwidGVtcGVyYXR1cmUiOjAuNywibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6IiIsInByb21wdCI6IiIsIm1vZGVsIjoiZ3B0LTQiLCJ1bmRlZmluZWQiOiIwLjIifQ%3D%3D"
    style={{width:"100%", height:"1250px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

### Segundo intento: De menos a m치s, primer paso - Reducci칩n

Aqu칤, trabajamos con 2 diferentes prompts. El primer prompt se utiliza para reducir el problema de entrada a una secuencia de pasos m치s simple. El segundo prompt se utiliza para mapear esta secuencia simplificada de pasos en acciones reales.

Ambos prompts son bastante largos y usan notaci칩n de Python comprimida para la acci칩n para ahorrar tokens.

El primer paso descompone la descripci칩n del lenguaje natural en un lenguaje m치s expl칤cito, pero a칰n humano. Esto ayudar치 al paso de mapeo a entender las cosas en secuencia.
Por ejemplo, "saltar hacia la izquierda dos veces" se reduce a "saltar a la izquierda" -> `TURN_LEFT + JUMP` y "saltar alrededor a la izquierda" -> `(TURN_LEFT + JUMP) * 4`. Del mismo modo, el paso de reducci칩n es el que se utiliza para explicar el concepto de repetici칩n (dos veces, tres veces, etc...).

<iframe
    src="http://embed.learnprompting.org/embed?config=eyJib3hSb3dzIjoyNSwidG9wUCI6MSwidGVtcGVyYXR1cmUiOjAuNywibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6IiIsInByb21wdCI6IiIsIm1vZGVsIjoiZ3B0LTQiLCJ1bmRlZmluZWQiOiIwLjIifQ%3D%3D"
    style={{width:"100%", height:"1250px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

### Segundo intento: De menor a mayor, segundo paso - Cartograf칤a

En el segundo paso, utilizamos el resultado de la reducci칩n y, de nuevo, una instrucci칩n bastante larga (14 casos) para traducir la descripci칩n reducida en lenguaje natural en una secuencia de acciones.

Aqu칤 inyectamos el resultado del primer paso:

> "saltar dos veces a la izquierda" puede resolverse con: "saltar a la izquierda", "saltar alrededor de la izquierda", "saltar alrededor de la izquierda dos veces". "camina hacia la izquierda tres veces" puede ser resuelto por: "caminar opuesto a la izquierda", "caminar opuesto a la izquierda tres veces". Por lo tanto, "saltar alrededor de la izquierda dos veces despu칠s de caminar frente a la izquierda tres veces" se puede resolver por: "saltar a la izquierda", "saltar alrededor de la izquierda", "saltar alrededor de la izquierda dos veces", "caminar opuesto a la izquierda", "caminar opuesto a la izquierda tres veces".

en el LLM.

<iframe
    src="http://embed.learnprompting.org/embed?config=eyJib3hSb3dzIjoyNSwidG9wUCI6MSwidGVtcGVyYXR1cmUiOjAuNywibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6IiIsInByb21wdCI6IiIsIm1vZGVsIjoiZ3B0LTQiLCJ1bmRlZmluZWQiOiIwLjIifQ%3D%3D"
    style={{width:"100%", height:"1250px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

### Resultados

LtM conlleva m칰ltiples mejoras:

- mejora de la precisi칩n con respecto a Chain of Thought
- mayor generalizaci칩n en problemas m치s dif칤ciles que los del prompt
- mejora espectacular del rendimiento en la generalizaci칩n composicional, en particular en la prueba de referencia SCAN (@lake2018scan).

Las instrucciones est치ndar con texto-davinci-002 (el modelo utilizado en el art칤culo) dan como resultado un 6% de problemas SCAN resueltos con 칠xito, mientras que las instrucciones de menor a mayor dan como resultado una impresionante tasa de 칠xito del 76%. Los resultados son a칰n m치s significativos con code-davinci-002, donde Least to Most logra una tasa de 칠xito del 99.7%.
