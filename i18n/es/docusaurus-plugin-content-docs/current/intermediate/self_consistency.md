---
sidebar_position: 5
---

# 游리 Autoconsistencia

Autoconsistencia(@wang2022selfconsistency) es un seguimiento de %%CoT|prompting de CoT%% que genera
m칰ltiples cadenas de pensamiento en lugar de solo una, luego toma la respuesta mayoritaria
como la respuesta final.

En la figura a continuaci칩n, el prompt de la izquierda est치 escrito utilizando el paradigma Few-Shot-CoT.
Usando este prompt, se generan m칰ltiples cadenas de pensamiento de manera independiente.
Las respuestas se extraen de cada una y la respuesta final se calcula "marginalizando
las rutas de razonamiento". En la pr치ctica, esto significa tomar la respuesta mayoritaria.

import SCImage from '@site/docs/assets/reliability/self_consistency.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={SCImage} style={{width: "750px"}} />
</div>

<div style={{textAlign: 'center'}}>
Autoconsistencia (Wang et al.)
</div>

## Ejemplo

Consideremos un ejemplo sencillo de an치lisis de correos electr칩nicos. Supongamos que eres una empresa de software y recibes cientos de correos electr칩nicos al d칤a. Quieres usar un modelo para clasificar los correos electr칩nicos como importantes o no importantes, para que puedas priorizar aquellos que puedan tener un gran impacto en tu negocio.

Aqu칤 hay un ejemplo de correo electr칩nico que podr칤as recibir:

```
Hola,

He descubierto una gran vulnerabilidad de seguridad en su sistema. Aunque no es f치cil de usar, es posible acceder a los datos de todos sus usuarios. He adjuntado una prueba de concepto. Por favor, solucione este problema lo antes posible.

Saludos cordiales,

Donny
```

Vamos a poner eso en un prompt:

```
Hola,

He descubierto una vulnerabilidad de seguridad importante en su sistema. Aunque no es f치cil de usar, es posible obtener acceso a todos los datos de sus usuarios. Adjunto una prueba de concepto. Por favor, solucione este problema lo antes posible.

Saludos,

Donny

Clasifique el correo electr칩nico anterior como IMPORTANTE o NO IMPORTANTE en relaci칩n con una empresa de software. Pensemos paso a paso.
```

Ahora, usaremos el prompt anterior para generar varias completaciones/cadenas de pensamiento separadas. Aqu칤 hay algunos ejemplos:

```text
<!-- highlight-start -->
- El remitente ha descubierto una vulnerabilidad de seguridad importante en el sistema. Esto es importante porque podr칤a conducir a una violaci칩n de datos. Clasificar칤a este correo electr칩nico como IMPORTANTE.
<!-- highlight-end -->
```

```text
<!-- highlight-start -->

- El remitente ha descubierto una vulnerabilidad de seguridad importante en el sistema, pero no ha proporcionado ninguna prueba de concepto y no es f치cil de usar. Clasificar칤a este correo electr칩nico como NO IMPORTANTE.
<!-- highlight-end -->
```

```text
<!-- highlight-start -->

- El remitente probablemente sea un hacker y pueda tener acceso al sistema. Esto es importante porque podr칤a conducir a una violaci칩n de datos. Clasificar칤a este correo electr칩nico como IMPORTANTE.
<!-- highlight-end -->
```

Al generar m칰ltiples cadenas de pensamiento y tomar la respuesta m치s com칰n (`IMPORTANTE`), podemos obtener una respuesta m치s consistentemente correcta de GPT-3.

## Resultados

Self-consistency ha demostrado mejorar los resultados en tareas de razonamiento aritm칠tico, de sentido com칰n y simb칩lico.

Incluso cuando se encontr칩 que CoT regular era ineficaz (@ye2022unreliability), self-consistency todav칤a fue capaz de mejorar los resultados.

## Notas

Wang et al. discuten un m칠todo m치s complejo para marginalizar los caminos de razonamiento, que se ocupa de las probabilidades generadas por LLM para cada cadena de pensamiento. Sin embargo, no utilizan este m칠todo en sus experimentos, y la votaci칩n mayoritaria parece tener generalmente el mismo o mejor rendimiento.
