---
sidebar_position: 5
---

# üü° Autoconsistencia

La autoconsistencia(@wang2022selfconsistency) (Self-consistency en ingles) es un seguimiento de la iniciativa CoT que genera m√∫ltiples cadenas de pensamiento en lugar de solo una, y luego toma la respuesta mayoritaria como respuesta final.

En la figura a continuaci√≥n, el prompt en la izquierda est√° escrito utilizando el paradigma Few-Shot-CoT. Usando este solo prompt, se generan m√∫ltiples cadenas de pensamiento de manera independiente. Las respuestas se extraen de cada una y la respuesta final se calcula "marginalizando los caminos de razonamiento". En la pr√°ctica, esto simplemente significa tomar la respuesta mayoritaria.

import SCImage from '@site/docs/assets/self_consistency.png';

<div style={{textAlign: 'center'}}>
  <img src={SCImage} style={{width: "750px"}} />
</div>

<div style={{textAlign: 'center'}}>
Autoconsistencia (Wang et al.)
</div>

## Resultados

Se ha demostrado que la autoconsistencia mejora los resultados en tareas de aritm√©tica, razonamiento com√∫n y simb√≥lico.

Incluso cuando se encontr√≥ que CoT regular era ineficaz (@ye2022unreliability), la autoconsistencia todav√≠a pudo mejorar los resultados.

## Notas

Wang et al. discuten un m√©todo m√°s complejo para marginalizar los caminos de razonamiento, que se ocupa de las probabilidades generadas por LLM para cada cadena de pensamiento. Sin embargo, no usan este m√©todo en sus experimentos, y la votaci√≥n mayoritaria parece tener el mismo o mejor rendimiento en general.