---
sidebar_position: 1
---

# 游댮 Soft Prompts

La sintonizaci칩n de prompts (@lester2021power), una alternativa a la sintonizaci칩n fina del modelo (@khashabi2021prompt), congela los pesos del modelo y actualiza los par치metros de un prompt. El prompt resultante es un 'prompt suave'.


import Image from '@site/docs/assets/trainable/prompt_tuning.webp';

<div style={{textAlign: 'center'}}>
  <img src={Image} style={{width: "500px"}} />
</div>

<div style={{textAlign: 'center'}}>
Ajuste del modelo vs. Ajuste del prompt (Lester et al.)
</div>

La imagen anterior contrasta la sintonizaci칩n del modelo con la sintonizaci칩n del prompt. En la sintonizaci칩n del modelo, se ajusta el mismo modelo en diferentes tareas. Esto te da unos pocos modelos diferentes, con los cuales no necesariamente puedes agrupar f치cilmente las entradas.

Por otro lado, la sintonizaci칩n del prompt te permite utilizar el mismo modelo para todas las tareas. S칩lo necesitas a침adir los prompts adecuados en el momento de la inferencia, lo que facilita el agrupamiento de diferentes tareas. B치sicamente, esto es la misma ventaja que tiene la sintonizaci칩n regular de prompts. Adem치s, los prompts suaves entrenados para un solo modelo en m칰ltiples tareas a menudo tendr치n la misma longitud de tokens.

## C칩mo funciona

Para entender la l칩gica b치sica detr치s de la sintonizaci칩n suave del prompt, pensemos en c칩mo funciona la **inferencia del modelo** en un prompt dado: "쮺u치nto es 2+2?".

1) Podr칤a ser tokenizado como "쮺u치nto, 'es', 2, +, 2,?".

2) Luego, cada token se convertir치 en un vector de valores.

3) Estos vectores de valores pueden considerarse como par치metros del modelo. El modelo puede ser adicionalmente entrenado, ajustando s칩lo los pesos de estos prompts.

N칩tese que tan pronto como empezamos a actualizar estos pesos, los vectores de los tokens ya no corresponden a los embeddings reales del vocabulario.

# Resultados

La sintonizaci칩n de prompts funciona mejor con modelos m치s grandes. Los modelos m치s grandes tambi칠n requieren menos tokens suaves del prompt. Sin embargo, m치s de 20 tokens no produce ganancias significativas de rendimiento.