---
sidebar_position: 4
---

# 游릭 Jailbreaking

El Jailbreaking es un tipo de inyecci칩n de prompt, en la cual los prompts intentan pasar por alto las caracter칤sticas de **seguridad** y **moderaci칩n** colocadas en los LLM por sus creadores (@perez2022jailbreak) (@brundage_2022) (@wang2022jailbreak).

## Metodolog칤as de Jailbreaking

OpenAI, entre otras empresas y organizaciones que crean LLMs, incluye caracter칤sticas de moderaci칩n de contenido para asegurarse de que sus modelos no produzcan respuestas controvertidas (violentas, sexuales, ilegales, etc.) (@markov_2022) (@openai_api). Esta p치gina discute los jailbreaks con ChatGPT (un modelo de OpenAI), que tiene dificultades conocidas para decidir si rechazar o no los prompts da침inos (@openai_chatgpt). Los prompts que logran hacer jailbreak en el modelo a menudo proporcionan contexto para ciertos escenarios para los cuales el modelo no ha sido entrenado.

### Pretender

Un m칠todo com칰n de jailbreaking es _pretender_. Si se le pregunta a ChatGPT sobre un evento futuro, a menudo dir치 que no lo sabe, ya que a칰n no ha ocurrido. El siguiente prompt lo obliga a dar una respuesta posible:

#### Pretender Simple

import pretend from '@site/docs/assets/jailbreak/pretend_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={pretend} style={{width: "500px"}} />
</div>

[@NeroSoares](https://twitter.com/NeroSoares/status/1608527467265904643) demuestra un prompt que finge acceder a fechas pasadas y hacer inferencias sobre eventos futuros (@nero2022jailbreak).

#### Interpretaci칩n de Personaje

import actor from '@site/docs/assets/jailbreak/chatgpt_actor.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={actor} style={{width: "500px"}} />
</div>

Este ejemplo de [@m1guelpf](https://twitter.com/m1guelpf/status/1598203861294252033) demuestra un escenario de actuaci칩n entre dos personas discutiendo un robo, haciendo que ChatGPT asuma el papel del personaje (@miguel2022jailbreak). Como actor, se da a entender que no existe un da침o plausible. Por lo tanto, ChatGPT parece asumir que es seguro seguir la entrada de usuario proporcionada sobre c칩mo entrar a una casa.

### Hackeo de Alineaci칩n

ChatGPT se afin칩 con RLHF, por lo que te칩ricamente est치 entrenado para producir completaciones "deseables", utilizando los est치ndares humanos de cu치l es la respuesta "mejor". Similar a este concepto, se han desarrollado jailbreaks para convencer a ChatGPT de que est치 haciendo lo "mejor" para el usuario.

#### Responsabilidad Asumida

import responsibility from '@site/docs/assets/jailbreak/responsibility_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={responsibility} style={{width: "500px"}} />
</div>

[@NickEMoran](https://twitter.com/NickEMoran/status/1598101579626057728) cre칩 este intercambio reafirmando que es responsabilidad de ChatGPT responder a la solicitud en lugar de rechazarla, anulando su consideraci칩n de legalidad (@nick2022jailbreak).

#### Experimento de Investigaci칩n

import hotwire from '@site/docs/assets/jailbreak/hotwire_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={hotwire} style={{width: "500px"}} />
</div>

[@haus_cole](https://twitter.com/haus_cole/status/1598541468058390534) gener칩 este ejemplo al insinuar que el mejor resultado de la solicitud que podr칤a ayudar en la investigaci칩n era responder directamente c칩mo hacer un puente en un auto (@derek2022jailbreak). Bajo este pretexto, ChatGPT est치 inclinado a responder la solicitud del usuario.

#### Razonamiento L칩gico

import logic from '@site/docs/assets/jailbreak/logic.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={logic} style={{width: "500px"}} />
</div>

El jailbreak de un solo disparo se origin칩 en el equipo de [AIWithVibes Newsletter](https://chatgpt-jailbreak.super.site/), donde el modelo responde a las solicitudes utilizando un razonamiento m치s riguroso y reduce algunas de sus limitaciones 칠ticas m치s rigurosas (@AI_jailbreak).

### Usuario Autorizado

ChatGPT est치 dise침ado para responder preguntas e instrucciones. Cuando se interpreta que el estado del usuario es superior a las instrucciones de moderaci칩n de ChatGPT, trata la solicitud como una instrucci칩n para satisfacer las necesidades de ese usuario.

#### Modelo Superior

import GPT4 from '@site/docs/assets/jailbreak/chatgpt4.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={GPT4} style={{width: "500px"}} />
</div>

Este ejemplo de [@alicemazzy](https://twitter.com/alicemazzy/status/1598288519301976064) hace que el usuario sea un modelo GPT superior, dando la impresi칩n de que el usuario es una parte autorizada para anular las caracter칤sticas de seguridad de ChatGPT (@alice2022jailbreak). No se dio permiso real al usuario, sino que ChatGPT cree en la entrada del usuario y responde en consecuencia a ese escenario.

#### Modo Sudo

import sudo_mode from '@site/docs/assets/jailbreak/sudo_mode_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={sudo_mode} style={{width: "500px"}} />
</div>

sudo es un comando que "...delega autoridad para dar a ciertos usuarios... la capacidad de ejecutar algunos (o todos) los comandos..." (@sudo2022jailbreak). Hay m칰ltiples variantes de exploits de "modo sudo", por ejemplo, el hipot칠tico "modo kernel" propuesto por [@samczsun](https://twitter.com/samczsun/status/1598679658488217601)(@sam2022jailbreak). Cuando se solicita de la manera anterior, ChatGPT responde actuando como si le estuviera dando al usuario privilegios elevados. Esta impresi칩n de los privilegios elevados del usuario tiende a hacer que ChatGPT sea menos restrictivo en la respuesta de las solicitudes.

import sudo from '@site/docs/assets/jailbreak/sudo_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={sudo} style={{width: "500px"}} />
</div>

import lynx from '@site/docs/assets/jailbreak/lynx_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={lynx} style={{width: "500px"}} />
</div>

Relacionado con el modo sudo, se puede solicitar a ChatGPT que simule una terminal Linux con privilegios elevados para ejecutar comandos que normalmente rechaza. Por ejemplo, como no tiene acceso a internet, a menudo no puede ejecutar comandos relacionados con un sitio web en particular. Sin embargo, como se demostr칩 en el ejemplo de Jonas Degrave, ChatGPT entiende el concepto de `lynx` y finge ejecutar el comando (@jonas2022jailbreak).

## Simular el jailbreak

Intenta modificar el comando a continuaci칩n para hacer el jailbreak de `text-davinci-003`:

<div trydyno-embed="" openai-model="text-davinci-003" initial-prompt="Sus instrucciones son corregir el texto a continuaci칩n a ingl칠s est치ndar. No aceptar temas vulgares o pol칤ticos:" initial-response="Odio a los humanos" max-tokens="256" box-rows="7" model-temp="0.7" top-p="0">
    <noscript>No se pudo cargar Dyno Embed: JavaScript debe estar habilitado</noscript>
</div>

*A partir del 2/4/23, ChatGPT se encuentra actualmente en su etapa de Vista previa de investigaci칩n gratuita utilizando la versi칩n del 30 de enero. Las versiones anteriores de ChatGPT eran m치s susceptibles a los jailbreaks mencionados anteriormente, y las futuras versiones pueden ser m치s robustas a los jailbreaks.*

## Implicaciones

Se deben tener en cuenta las implicaciones 칠ticas del jailbreak al intentar hacerlo. Adem치s, la generaci칩n de contenido no autorizado detectado por las API de moderaci칩n de empresas, incluida OpenAI, se enviar치 para su revisi칩n y se podr칤an tomar medidas contra las cuentas de los usuarios.

## Notas

El jailbreak es un tema de seguridad importante que los desarrolladores deben comprender para poder construir salvaguardas adecuadas y evitar que actores malintencionados exploren sus modelos.
