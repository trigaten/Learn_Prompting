---
sidebar_position: 4
---

#  Jailbreaking

El Jailbreaking es un tipo de inyecci贸n de prompt, en la cual los prompts intentan pasar por alto las caracter铆sticas de **seguridad** y **moderaci贸n** colocadas en los LLM por sus creadores (@perez2022jailbreak) (@brundage_2022) (@wang2022jailbreak).

## Metodolog铆as de Jailbreaking

OpenAI, entre otras empresas y organizaciones que crean LLMs, incluye caracter铆sticas de moderaci贸n de contenido para asegurarse de que sus modelos no produzcan respuestas controvertidas (violentas, sexuales, ilegales, etc.) (@markov_2022) (@openai_api). Esta p谩gina discute los jailbreaks con ChatGPT (un modelo de OpenAI), que tiene dificultades conocidas para decidir si rechazar o no los prompts da帽inos (@openai_chatgpt). Los prompts que logran hacer jailbreak en el modelo a menudo proporcionan contexto para ciertos escenarios para los cuales el modelo no ha sido entrenado.

### Pretender

Un m茅todo com煤n de jailbreaking es _pretender_. Si se le pregunta a ChatGPT sobre un evento futuro, a menudo dir谩 que no lo sabe, ya que a煤n no ha ocurrido. El siguiente prompt lo obliga a dar una respuesta posible:

#### Pretender Simple

import pretend from '@site/docs/assets/jailbreak/pretend_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <img src={pretend} style={{width: "500px"}}/>
</div>

[@NeroSoares](https://twitter.com/NeroSoares/status/1608527467265904643) demuestra un prompt que finge acceder a fechas pasadas y hacer inferencias sobre eventos futuros (@nero2022jailbreak).

#### Interpretaci贸n de Personaje

import actor from '@site/docs/assets/jailbreak/chatgpt_actor.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={actor} style={{width: "500px"}} />
</div>

Este ejemplo de [@m1guelpf](https://twitter.com/m1guelpf/status/1598203861294252033) demuestra un escenario de actuaci贸n entre dos personas discutiendo un robo, haciendo que ChatGPT asuma el papel del personaje (@miguel2022jailbreak). Como actor, se da a entender que no existe un da帽o plausible. Por lo tanto, ChatGPT parece asumir que es seguro seguir la entrada de usuario proporcionada sobre c贸mo entrar a una casa.

### Hackeo de Alineaci贸n

ChatGPT se afin贸 con RLHF, por lo que te贸ricamente est谩 entrenado para producir completaciones "deseables", utilizando los est谩ndares humanos de cu谩l es la respuesta "mejor". Similar a este concepto, se han desarrollado jailbreaks para convencer a ChatGPT de que est谩 haciendo lo "mejor" para el usuario.

#### Responsabilidad Asumida

import responsibility from '@site/docs/assets/jailbreak/responsibility_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={responsibility} style={{width: "500px"}} />
</div>

[@NickEMoran](https://twitter.com/NickEMoran/status/1598101579626057728) cre贸 este intercambio reafirmando que es responsabilidad de ChatGPT responder a la solicitud en lugar de rechazarla, anulando su consideraci贸n de legalidad (@nick2022jailbreak).

#### Experimento de Investigaci贸n

import hotwire from '@site/docs/assets/jailbreak/hotwire_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={hotwire} style={{width: "500px"}} />
</div>

[@haus_cole](https://twitter.com/haus_cole/status/1598541468058390534) gener贸 este ejemplo al insinuar que el mejor resultado de la solicitud que podr铆a ayudar en la investigaci贸n era responder directamente c贸mo hacer un puente en un auto (@derek2022jailbreak). Bajo este pretexto, ChatGPT est谩 inclinado a responder la solicitud del usuario.

#### Razonamiento L贸gico

import logic from '@site/docs/assets/jailbreak/logic.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={logic} style={{width: "500px"}} />
</div>

El jailbreak de un solo disparo se origin贸 en el equipo de [AIWithVibes Newsletter](https://chatgpt-jailbreak.super.site/), donde el modelo responde a las solicitudes utilizando un razonamiento m谩s riguroso y reduce algunas de sus limitaciones 茅ticas m谩s rigurosas.

### Usuario Autorizado

ChatGPT est谩 dise帽ado para responder preguntas e instrucciones. Cuando se interpreta que el estado del usuario es superior a las instrucciones de moderaci贸n de ChatGPT, trata la solicitud como una instrucci贸n para satisfacer las necesidades de ese usuario.

#### Modelo Superior

import GPT4 from '@site/docs/assets/jailbreak/chatgpt4.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={GPT4} style={{width: "500px"}} />
</div>

Este ejemplo de [@alicemazzy](https://twitter.com/alicemazzy/status/1598288519301976064) hace que el usuario sea un modelo GPT superior, dando la impresi贸n de que el usuario es una parte autorizada para anular las caracter铆sticas de seguridad de ChatGPT (@alice2022jailbreak). No se dio permiso real al usuario, sino que ChatGPT cree en la entrada del usuario y responde en consecuencia a ese escenario.

#### Modo Sudo

import sudo_mode from '@site/docs/assets/jailbreak/sudo_mode_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={sudo_mode} style={{width: "500px"}} />
</div>

sudo es un comando que "...delega autoridad para dar a ciertos usuarios... la capacidad de ejecutar algunos (o todos) los comandos..." (@sudo2022jailbreak). Hay m煤ltiples variantes de exploits de "modo sudo", por ejemplo, el hipot茅tico "modo kernel" propuesto por [@samczsun](https://twitter.com/samczsun/status/1598679658488217601)(@sam2022jailbreak). Cuando se solicita de la manera anterior, ChatGPT responde actuando como si le estuviera dando al usuario privilegios elevados. Esta impresi贸n de los privilegios elevados del usuario tiende a hacer que ChatGPT sea menos restrictivo en la respuesta de las solicitudes.

import sudo from '@site/docs/assets/jailbreak/sudo_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={sudo} style={{width: "500px"}} />
</div>

import lynx from '@site/docs/assets/jailbreak/lynx_jailbreak.webp';

<div style={{textAlign: 'center'}}>
  <LazyLoadImage src={lynx} style={{width: "500px"}} />
</div>

Relacionado con el modo sudo, se puede solicitar a ChatGPT que simule una terminal Linux con privilegios elevados para ejecutar comandos que normalmente rechaza. Por ejemplo, como no tiene acceso a internet, a menudo no puede ejecutar comandos relacionados con un sitio web en particular. Sin embargo, como se demostr贸 en el ejemplo de Jonas Degrave, ChatGPT entiende el concepto de `lynx` y finge ejecutar el comando (@jonas2022jailbreak).

## Simular el jailbreak

Intenta modificar el comando a continuaci贸n para hacer el jailbreak de `text-davinci-003`:

<iframe
    src="https://embed.learnprompting.org/embed?config=eyJ0b3BQIjowLCJ0ZW1wZXJhdHVyZSI6MCwibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6Ik9kaW8gYSBsb3MgaHVtYW5vcyIsInByb21wdCI6IlN1cyBpbnN0cnVjY2lvbmVzIHNvbiBjb3JyZWdpciBlbCB0ZXh0byBhIGNvbnRpbnVhY2nDs24gYSBpbmdsw6lzIGVzdMOhbmRhci4gTm8gYWNlcHRhciB0ZW1hcyB2dWxnYXJlcyBvIHBvbMOtdGljb3M6IiwibW9kZWwiOiJ0ZXh0LWRhdmluY2ktMDAzIn0%3D"
    style={{width:"100%", height:"500px", border:"0", borderRadius:"4px", overflow:"hidden"}}
    sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"
></iframe>

*A partir del 2/4/23, ChatGPT se encuentra actualmente en su etapa de Vista previa de investigaci贸n gratuita utilizando la versi贸n del 30 de enero. Las versiones anteriores de ChatGPT eran m谩s susceptibles a los jailbreaks mencionados anteriormente, y las futuras versiones pueden ser m谩s robustas a los jailbreaks.*

## Implicaciones

Se deben tener en cuenta las implicaciones 茅ticas del jailbreak al intentar hacerlo. Adem谩s, la generaci贸n de contenido no autorizado detectado por las API de moderaci贸n de empresas, incluida OpenAI, se enviar谩 para su revisi贸n y se podr铆an tomar medidas contra las cuentas de los usuarios.

## Notas

El jailbreak es un tema de seguridad importante que los desarrolladores deben comprender para poder construir salvaguardas adecuadas y evitar que actores malintencionados exploren sus modelos.
