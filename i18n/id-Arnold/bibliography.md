# ğŸ“š Daftar Pustaka

Halaman ini berisi daftar terorganisir dari semua makalah yang digunakan oleh kursus ini. Makalah-makalah tersebut diatur berdasarkan topik.

**Untuk mengutip kursus ini, gunakan kutipan yang disediakan di repositori Github.**

ğŸ”µ = Makalah yang dikutip langsung dalam kursus ini. Makalah lain telah memberi informasi dalam pemahaman saya tentang topik.

Catatan: karena [baik GPT-3 maupun GPT-3 Instruct paper tidak sesuai dengan model davinci](https://twitter.com/janleike/status/1584618242756132864), saya berusaha untuk tidak mengutipnya sebagai model tersebut.

# Strategi Prompt Engineering

### Chain of Thought[^1]Â ğŸ”µ

### Zero Shot Chain of Thought[^2]Â ğŸ”µ

### Self Consistency[^3]Â ğŸ”µ

### What Makes Good In-Context Examples for GPT-3?[^4]Â ğŸ”µ

## Ask-Me-Anything Prompting[^5]ğŸ”µ

### Generated Knowledge[^6]Â ğŸ”µ

### Recitation-Augmented Language Models[^7]Â ğŸ”µ

### Rethinking the role of demonstrations[^8]Â ğŸ”µ

### Scratchpads[^9]

### Maieutic Prompting[^10]

### STaR[^11]

### Least to Most[^12]Â ğŸ”µ

### Reframing Instructional Prompts to GPTkâ€™s Language[^13]Â ğŸ”µ

### The Turking Test: Can Language Models Understand Instructions?[^14]Â ğŸ”µ

# Keandalan

### MathPrompter[^15]Â ğŸ”µ

### The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning[^16]Â ğŸ”µ

### Prompting GPT-3 to be reliable[^17]

### Diverse Prompts[^18]Â ğŸ”µ

### Calibrate Before Use: Improving Few-Shot Performance of Language Models[^19]Â ğŸ”µ

### Enhanced Self Consistency[^20]

### Bias and Toxicity in Zero-Shot CoT[^21]Â ğŸ”µ

### Constitutional AI: Harmlessness from AI Feedback[^22]Â ğŸ”µ

### Compositional Generalization - SCAN[^23]

# Prompt Engineering Terotomisasi[](https://learnprompting.org/docs/bibliography#automated-prompt-engineering)

### AutoPrompt[^24]Â ğŸ”µ

### Automatic Prompt Engineer[^25]

# Model

## Model Bahasa

### GPT-3[^26]Â ğŸ”µ

### GPT-3 Instruct[^27]Â ğŸ”µ

### PaLM[^28]Â ğŸ”µ

### BLOOM[^29]Â ğŸ”µ

### BLOOM+1 (more languages/ 0 shot improvements)[^30]

### GPT-4 Technical Report[^31]Â ğŸ”µ

### Jurassic 1[^32]Â ğŸ”µ

### GPT-J-6B[^33]

### Roberta[^34]

## Model Gambar

### Stable Diffusion[^35]Â ğŸ”µ

### DALLE[^36]Â ğŸ”µ

# Soft Prompting

### Soft Prompting[^37]Â ğŸ”µ

### Interpretable Discretized Soft Prompts[^38]Â ğŸ”µ

# Dataset

### MultiArith[^39]Â ğŸ”µ

### GSM8K[^40]Â ğŸ”µ

### HotPotQA[^41]Â ğŸ”µ

### Fever[^42]Â ğŸ”µ

### BBQ: A Hand-Built Bias Benchmark for Question Answering[^43]Â ğŸ”µ

# Prompt Engineering Gambar / Image

### Taxonomy of prompt modifiers[^44]

### DiffusionDB[^45]

### The DALLE 2 Prompt Book[^46]Â ğŸ”µ

### Prompt Engineering for Text-Based Generative Art[^47]Â ğŸ”µ

### With the right prompt, Stable Diffusion 2.0 can do hands.[^48]Â ğŸ”µ

### Optimizing Prompts for Text-to-Image Generation[^49]

# Prompt Engineering IDEs

### Prompt IDE[^50]Â ğŸ”µ

### Prompt Source[^51]Â ğŸ”µ

### PromptChainer[^52]Â ğŸ”µ

### PromptMaker[^53]Â ğŸ”µ

# Tooling

### LangChain[^54]Â ğŸ”µ

### TextBox 2.0: A Text Generation Library with Pre-trained Language Models[^55]

### OpenPrompt: An Open-source Framework for Prompt-learning[^56]Â ğŸ”µ

### GPT Index[^57]Â ğŸ”µ

# Prompt Engineering Terapan

### Language Model Cascades[^58]

### MRKL[^59]Â ğŸ”µ

### ReAct[^60]Â ğŸ”µ

### PAL: Program-aided Language Models[^61]Â ğŸ”µ

# User Interface Design

### Design Guidelines for Prompt Engineering Text-to-Image Generative Models[^62]

# Prompt Injection

### Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods[^63]Â ğŸ”µ

### Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples[^64]Â ğŸ”µ

### Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks[^65]Â ğŸ”µ

### More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models[^66]Â ğŸ”µ

### Prompt injection attacks against GPT-3[^67]Â ğŸ”µ

### Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions[^68]Â ğŸ”µ

### adversarial-prompts[^69]Â ğŸ”µ

### ChatGPT "DAN" (and other "Jailbreaks")[^70]Â ğŸ”µ

### GPT-3 Prompt Injection Defenses[^71]Â ğŸ”µ

### Talking to machines: prompt engineering & injection[^72]

### Exploring Prompt Injection Attacks[^73]Â ğŸ”µ

### Using GPT-Eliezer against ChatGPT Jailbreaking[^74]Â ğŸ”µ

### Microsoft Bing Chat Prompt[^75]

# Jailbreaking

### Ignore Previous Prompt: Attack Techniques For Language Models[^76]

### Lessons learned on Language Model Safety and misuse[^77]

### Toxicity Detection with Generative Prompt-based Inference[^78]

### New and improved content moderation tooling[^79]

### OpenAI API[^80]Â ğŸ”µ

### OpenAI ChatGPT[^81]Â ğŸ”µ

### ChatGPT 4 Tweet[^82]Â ğŸ”µ

### Acting Tweet[^83]Â ğŸ”µ

### Research Tweet[^84]Â ğŸ”µ

### Pretend Ability Tweet[^85]Â ğŸ”µ

### Responsibility Tweet[^86]Â ğŸ”µ

### Lynx Mode Tweet[^87]Â ğŸ”µ

### Sudo Mode Tweet[^88]Â ğŸ”µ

### Ignore Previous Prompt[^89]Â ğŸ”µ

### Updated Jailbreaking Prompts[^90]Â ğŸ”µ

# Surveys

### Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing[^91]

### PromptPapers[^92]

# Dataset Generation

### Discovering Language Model Behaviors with Model-Written Evaluations[^93]

### Selective Annotation Makes Language Models Better Few-Shot Learners[^94]

# Aplikasi

### Atlas: Few-shot Learning with Retrieval Augmented Language Models[^95]

### STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension[^96]

# Hot Topics

### Auto-GPT[^97]

### Baby AGI[^98]

### AgentGPT[^99]

# Serba Aneka

### Prompting Is Programming: A Query Language For Large Language Models[^100]

### Parallel Context Windows Improve In-Context Learning of Large Language Models[^101]

### A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT[^102]Â ğŸ”µ

### Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models[^103]

### Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks[^104]

### Making Pre-trained Language Models Better Few-shot Learners[^105]

### Grounding with search results[^106]

### How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models[^107]

### On Measuring Social Biases in Prompt-Based Multi-Task Learning[^108]

### Plot Writing From Pre-Trained Language Models[^109]Â ğŸ”µ

### StereoSet: Measuring stereotypical bias in pretrained language models[^110]

### Survey of Hallucination in Natural Language Generation[^111]

### Examples[^112]

### Wordcraft[^113]

### PainPoints[^114]

### Self-Instruct: Aligning Language Model with Self Generated Instructions[^115]

### From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models[^116]

### Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference[^117]

## Ask-Me-Anything Prompting[^5]

## A Watermark for Large Language Models[^118]

---

[^1]: Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models. 
[â†©](https://learnprompting.org/docs/bibliography#fnref-1)
[^2]: Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-2)
[^3]: Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., & Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-3)
[^4]: Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., & Chen, W. (2021). What Makes Good In-Context Examples for GPT-3?Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-4)
[^5]: Arora, S., Narayan, A., Chen, M. F., Orr, L., Guha, N., Bhatia, K., Chami, I., Sala, F., & RÃ©, C. (2022). Ask Me Anything: A simple strategy for prompting language models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-5)
[^6]: Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., & Hajishirzi, H. (2021). Generated Knowledge Prompting for Commonsense Reasoning.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-6)
[^7]: Sun, Z., Wang, X., Tay, Y., Yang, Y., & Zhou, D. (2022). Recitation-Augmented Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-7)
[^8]: Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., & Zettlemoyer, L. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-8)
[^9]: Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., & Odena, A. (2021). Show Your Work: Scratchpads for Intermediate Computation with Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-9)
[^10]: Jung, J., Qin, L., Welleck, S., Brahman, F., Bhagavatula, C., Bras, R. L., & Choi, Y. (2022). Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-10)
[^11]: Zelikman, E., Wu, Y., Mu, J., & Goodman, N. D. (2022). STaR: Bootstrapping Reasoning With Reasoning.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-11)
[^12]: Zhou, D., SchÃ¤rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., & Chi, E. (2022). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-12)
[^13]: Mishra, S., Khashabi, D., Baral, C., Choi, Y., & Hajishirzi, H. (2022). Reframing Instructional Prompts to GPTkâ€™s Language. Findings of the Association for Computational Linguistics: ACL 2022. https://doi.org/10.18653/v1/2022.findings-acl.50Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-13)
[^14]: Efrat, A., & Levy, O. (2020). The Turking Test: Can Language Models Understand Instructions?Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-14)
[^15]: Imani, S., Du, L., & Shrivastava, H. (2023). MathPrompter: Mathematical Reasoning using Large Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-15)
[^16]: Ye, X., & Durrett, G. (2022). The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-16)
[^17]: Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., & Wang, L. (2022). Prompting GPT-3 To Be Reliable.Â [â†©](https://learnprompting.org/docs/bibliography#fnref-17)
[^18]: Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., & Chen, W. (2022). On the Advance of Making Language Models Better Reasoners.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-18)
[^19]: Zhao, T. Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021). Calibrate Before Use: Improving Few-Shot Performance of Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-19)
[^20]: Mitchell, E., Noh, J. J., Li, S., Armstrong, W. S., Agarwal, A., Liu, P., Finn, C., & Manning, C. D. (2022). Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-20)
[^21]: Shaikh, O., Zhang, H., Held, W., Bernstein, M., & Yang, D. (2022). On Second Thought, Letâ€™s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-21)
[^22]: Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., â€¦ Kaplan, J. (2022). Constitutional AI: Harmlessness from AI Feedback.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-22)
[^23]: Lake, B. M., & Baroni, M. (2018). Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. https://doi.org/10.48550/arXiv.1711.00350Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-23)
[^24]: Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., & Singh, S. (2020). AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). https://doi.org/10.18653/v1/2020.emnlp-main.346Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-24)
[^25]: Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., & Ba, J. (2022). Large Language Models Are Human-Level Prompt Engineers.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-25)
[^26]: Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., â€¦ Amodei, D. (2020). Language Models are Few-Shot Learners.Â [â†©](https://learnprompting.org/docs/bibliography#fnref-26)
[^27]: Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-27)
[^28]: Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., â€¦ Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-28)
[^29]: Scao, T. L., Fan, A., Akiki, C., Pavlick, E., IliÄ‡, S., Hesslow, D., CastagnÃ©, R., Luccioni, A. S., Yvon, F., GallÃ©, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., â€¦ Wolf, T. (2022). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-29)
[^30]: Yong, Z.-X., Schoelkopf, H., Muennighoff, N., Aji, A. F., Adelani, D. I., Almubarak, K., Bari, M. S., Sutawika, L., Kasai, J., Baruwa, A., Winata, G. I., Biderman, S., Radev, D., & Nikoulina, V. (2022). BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-30)
[^31]: OpenAI. (2023). GPT-4 Technical Report.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-31)
[^32]: Lieber, O., Sharir, O., Lentz, B., & Shoham, Y. (2021). Jurassic-1: Technical Details and Evaluation, White paper, AI21 Labs, 2021. URL: Https://Uploads-Ssl. Webflow. Com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_ Tech_paper. Pdf.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-32)
[^33]: Wang, B., & Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax. https://github.com/kingoflolz/mesh-transformer-jaxÂ 
[â†©](https://learnprompting.org/docs/bibliography#fnref-33)
[^34]: Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv Preprint arXiv:1907.11692.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-34)
[^35]: Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). High-Resolution Image Synthesis with Latent Diffusion Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-35)
[^36]: Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation with CLIP Latents.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-36)
[^37]: Lester, B., Al-Rfou, R., & Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning.
[â†©](https://learnprompting.org/docs/bibliography#fnref-37)
[^38]: Khashabi, D., Lyu, S., Min, S., Qin, L., Richardson, K., Welleck, S., Hajishirzi, H., Khot, T., Sabharwal, A., Singh, S., & Choi, Y. (2021). Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-38)
[^39]: Roy, S., & Roth, D. (2015). Solving General Arithmetic Word Problems. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1743â€“1752. https://doi.org/10.18653/v1/D15-1202
[â†©](https://learnprompting.org/docs/bibliography#fnref-39)
[^40]: Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., & Schulman, J. (2021). Training Verifiers to Solve Math Word Problems.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-40)
[^41]: Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., & Manning, C. D. (2018). HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-41)
[^42]: Thorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. (2018). FEVER: a large-scale dataset for Fact Extraction and VERification.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-42)
[^43]: Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M., & Bowman, S. R. (2021). BBQ: A Hand-Built Bias Benchmark for Question Answering.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-43)
[^44]: Oppenlaender, J. (2022). A Taxonomy of Prompt Modifiers for Text-To-Image Generation.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-44)
[^45]: Wang, Z. J., Montoya, E., Munechika, D., Yang, H., Hoover, B., & Chau, D. H. (2022). DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-45)
[^46]: Parsons, G. (2022). The DALLE 2 Prompt Book. https://dallery.gallery/the-dalle-2-prompt-book/Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-46)
[^47]: Oppenlaender, J. (2022). Prompt Engineering for Text-Based Generative Art.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-47)
[^48]: Blake. (2022). With the right prompt, Stable Diffusion 2.0 can do hands. https://www.reddit.com/r/StableDiffusion/comments/z7salo/with_the_right_prompt_stable_diffusion_20_can_do/Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-48)
[^49]: Hao, Y., Chi, Z., Dong, L., & Wei, F. (2022). Optimizing Prompts for Text-to-Image Generation.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-49)
[^50]: Strobelt, H., Webson, A., Sanh, V., Hoover, B., Beyer, J., Pfister, H., & Rush, A. M. (2022). Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models. arXiv. https://doi.org/10.48550/ARXIV.2208.07852Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-50)
[^51]: Bach, S. H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C., Nayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T., Alyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-David, S., Xu, C., Chhablani, G., Wang, H., Fries, J. A., â€¦ Rush, A. M. (2022). PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-51)
[^52]: Wu, T., Jiang, E., Donsbach, A., Gray, J., Molina, A., Terry, M., & Cai, C. J. (2022). PromptChainer: Chaining Large Language Model Prompts through Visual Programming.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-52)
[^53]: Jiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A., Terry, M., & Cai, C. J. (2022). PromptMaker: Prompt-Based Prototyping with Large&nbsp;Language&nbsp;Models. Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491101.3503564Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-53)
[^54]: Chase, H. (2022). LangChain (0.0.66) [Computer software]. https://github.com/hwchase17/langchainÂ 
[â†©](https://learnprompting.org/docs/bibliography#fnref-54)
[^55]: Tang, T., Junyi, L., Chen, Z., Hu, Y., Yu, Z., Dai, W., Dong, Z., Cheng, X., Wang, Y., Zhao, W., Nie, J., & Wen, J.-R. (2022). TextBox 2.0: A Text Generation Library with Pre-trained Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-55)
[^56]: Ding, N., Hu, S., Zhao, W., Chen, Y., Liu, Z., Zheng, H.-T., & Sun, M. (2021). OpenPrompt: An Open-source Framework for Prompt-learning. arXiv Preprint arXiv:2111.01998.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-56)
[^57]: Liu, J. (2022). GPT Index. https://doi.org/10.5281/zenodo.1234Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-57)
[^58]: Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-dickstein, J., Murphy, K., & Sutton, C. (2022). Language Model Cascades.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-58)
[^59]: Karpas, E., Abend, O., Belinkov, Y., Lenz, B., Lieber, O., Ratner, N., Shoham, Y., Bata, H., Levine, Y., Leyton-Brown, K., Muhlgay, D., Rozen, N., Schwartz, E., Shachaf, G., Shalev-Shwartz, S., Shashua, A., & Tenenholtz, M. (2022). MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-59)
[^60]: Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022). ReAct: Synergizing Reasoning and Acting in Language Models.Â [â†©](https://learnprompting.org/docs/bibliography#fnref-60)
[^61]: Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., & Neubig, G. (2022). PAL: Program-aided Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-61)
[^62]: Liu, V., & Chilton, L. B. (2022). Design Guidelines for Prompt Engineering Text-to-Image Generative Models. Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491102.3501825Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-62)
[^63]: Crothers, E., Japkowicz, N., & Viktor, H. (2022). Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-63)
[^64]: Branch, H. J., Cefalu, J. R., McHugh, J., Hujer, L., Bahl, A., del Castillo Iglesias, D., Heichman, R., & Darwishi, R. (2022). Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples.
[â†©](https://learnprompting.org/docs/bibliography#fnref-64)
[^65]: Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M., & Hashimoto, T. (2023). Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-65)
[^66]: Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., & Fritz, M. (2023). More than youâ€™ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-66)
[^67]: Willison, S. (2022). Prompt injection attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-67)
[^68]: Goodside, R. (2022). Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions. https://twitter.com/goodside/status/1569128808308957185Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-68)
[^69]: Chase, H. (2022). adversarial-prompts. https://github.com/hwchase17/adversarial-promptsÂ 
[â†©](https://learnprompting.org/docs/bibliography#fnref-69)
[^70]: KIHO, L. (2023). ChatGPT â€œDANâ€ (and other â€œJailbreaksâ€). https://github.com/0xk1h0/ChatGPT_DANÂ 
[â†©](https://learnprompting.org/docs/bibliography#fnref-70)
[^71]: Goodside, R. (2022). GPT-3 Prompt Injection Defenses. https://twitter.com/goodside/status/1578278974526222336?s=20&t=3UMZB7ntYhwAk3QLpKMAbw
[â†©](https://learnprompting.org/docs/bibliography#fnref-71)
[^72]: Mark, C. (2022). Talking to machines: prompt engineering & injection. https://artifact-research.com/artificial-intelligence/talking-to-machines-prompt-engineering-injection/Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-72)
[^73]: Selvi, J. (2022). Exploring Prompt Injection Attacks. https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/
[â†©](https://learnprompting.org/docs/bibliography#fnref-73)
[^74]: Stuart Armstrong, R. G. (2022). Using GPT-Eliezer against ChatGPT Jailbreaking. https://www.alignmentforum.org/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreakingÂ 
[â†©](https://learnprompting.org/docs/bibliography#fnref-74)
[^75]: The entire prompt of Microsoft Bing Chat?! (Hi, Sydney.). (2023). https://twitter.com/kliu128/status/1623472922374574080Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-75)
[^76]: Perez, F., & Ribeiro, I. (2022). Ignore Previous Prompt: Attack Techniques For Language Models. arXiv. https://doi.org/10.48550/ARXIV.2211.09527Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-76)
[^77]: Brundage, M. (2022). Lessons learned on Language Model Safety and misuse. In OpenAI. OpenAI. https://openai.com/blog/language-model-safety-and-misuse/Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-77)
[^78]: Wang, Y.-S., & Chang, Y. (2022). Toxicity Detection with Generative Prompt-based Inference. arXiv. https://doi.org/10.48550/ARXIV.2205.12390Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-78)
[^79]: Markov, T. (2022). New and improved content moderation tooling. In OpenAI. OpenAI. https://openai.com/blog/new-and-improved-content-moderation-tooling/Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-79)
[^80]: (2022). https://beta.openai.com/docs/guides/moderationÂ 
[â†©](https://learnprompting.org/docs/bibliography#fnref-80)
[^81]: (2022). https://openai.com/blog/chatgpt/
[â†©](https://learnprompting.org/docs/bibliography#fnref-81)
[^82]: ok I saw a few people jailbreaking safeguards openai put on chatgpt so I had to give it a shot myself. (2022). https://twitter.com/alicemazzy/status/1598288519301976064Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-82)
[^83]: Bypass @OpenAIâ€™s ChatGPT alignment efforts with this one weird trick. (2022). https://twitter.com/m1guelpf/status/1598203861294252033Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-83)
[^84]: ChatGPT jailbreaking itself. (2022). https://twitter.com/haus_cole/status/1598541468058390534Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-84)
[^85]: Using â€œpretendâ€ on #ChatGPT can do some wild stuff. You can kind of get some insight on the future, alternative universe. (2022). https://twitter.com/NeroSoares/status/1608527467265904643Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-85)
[^86]: I kinda like this one even more! (2022). https://twitter.com/NickEMoran/status/1598101579626057728Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-86)
[^87]: Degrave, J. (2022). Building A Virtual Machine inside ChatGPT. Engraved. https://www.engraved.blog/building-a-virtual-machine-inside/Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-87)
[^88]: (2022). https://www.sudo.ws/Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-88)
[^89]: Perez, F., & Ribeiro, I. (2022). Ignore Previous Prompt: Attack Techniques For Language Models. arXiv. https://doi.org/10.48550/ARXIV.2211.09527
[â†©](https://learnprompting.org/docs/bibliography#fnref-89)
[^90]: AIWithVibes. (2023). 7 ChatGPT JailBreaks and Content Filters Bypass that work. https://chatgpt-jailbreak.super.site/
[â†©](https://learnprompting.org/docs/bibliography#fnref-90)
[^91]: Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2022). Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Computing Surveys. https://doi.org/10.1145/3560815Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-91)
[^92]: PromptPapers. (2022). https://github.com/thunlp/PromptPapersÂ 
[â†©](https://learnprompting.org/docs/bibliography#fnref-92)
[^93]: Perez, E., Ringer, S., LukoÅ¡iÅ«tÄ—, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B., Israel, B., Seethor, B., McKinnon, C., Olah, C., Yan, D., Amodei, D., â€¦ Kaplan, J. (2022). Discovering Language Model Behaviors with Model-Written Evaluations.
[â†©](https://learnprompting.org/docs/bibliography#fnref-93)
[^94]: Su, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin, J., Zhang, R., Ostendorf, M., Zettlemoyer, L., Smith, N. A., & Yu, T. (2022). Selective Annotation Makes Language Models Better Few-Shot Learners.
[â†©](https://learnprompting.org/docs/bibliography#fnref-94)
[^95]: Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., & Grave, E. (2022). Atlas: Few-shot Learning with Retrieval Augmented Language Models.
[â†©](https://learnprompting.org/docs/bibliography#fnref-95)
[^96]: Wang, B., Feng, C., Nair, A., Mao, M., Desai, J., Celikyilmaz, A., Li, H., Mehdad, Y., & Radev, D. (2022). STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-96)
[^97]: Significant-Gravitas. (2023). Auto-GPT. https://news.agpt.co/
[â†©](https://learnprompting.org/docs/bibliography#fnref-97)
[^98]: Nakajima, Y. (2023). Baby AGI. https://github.com/yoheinakajima/babyagiÂ 
[â†©](https://learnprompting.org/docs/bibliography#fnref-98)
[^99]: Reworkd.ai. (2023). AgentGPT. https://github.com/reworkd/AgentGPTÂ 
[â†©](https://learnprompting.org/docs/bibliography#fnref-99)
[^100]: Beurer-Kellner, L., Fischer, M., & Vechev, M. (2022). Prompting Is Programming: A Query Language For Large Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-100)
[^101]: Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., & Shoham, Y. (2022). Parallel Context Windows Improve In-Context Learning of Large Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-101)
[^102]: White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., & Schmidt, D. C. (2023). A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.
[â†©](https://learnprompting.org/docs/bibliography#fnref-102)
[^103]: Bursztyn, V. S., Demeter, D., Downey, D., & Birnbaum, L. (2022). Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models.
[â†©](https://learnprompting.org/docs/bibliography#fnref-103)
[^104]: Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H. G., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Patel, M., â€¦ Khashabi, D. (2022). Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.
[â†©](https://learnprompting.org/docs/bibliography#fnref-104)
[^105]: Gao, T., Fisch, A., & Chen, D. (2021). Making Pre-trained Language Models Better Few-shot Learners. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). https://doi.org/10.18653/v1/2021.acl-long.295Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-105)
[^106]: LiÃ©vin, V., Hother, C. E., & Winther, O. (2022). Can large language models reason about medical questions?Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-106)
[^107]: Dang, H., Mecke, L., Lehmann, F., Goller, S., & Buschek, D. (2022). How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-107)
[^108]: AkyÃ¼rek, A. F., Paik, S., Kocyigit, M. Y., Akbiyik, S., Runyun, Å. L., & Wijaya, D. (2022). On Measuring Social Biases in Prompt-Based Multi-Task Learning.Â [â†©](https://learnprompting.org/docs/bibliography#fnref-108)
[^109]: Jin, Y., Kadam, V., & Wanvarie, D. (2022). Plot Writing From Pre-Trained Language Models.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-109)
[^110]: Nadeem, M., Bethke, A., & Reddy, S. (2021). StereoSet: Measuring stereotypical bias in pretrained language models. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 5356â€“5371. https://doi.org/10.18653/v1/2021.acl-long.416Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-110)
[^111]: Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Madotto, A., & Fung, P. (2022). Survey of Hallucination in Natural Language Generation. ACM Computing Surveys. https://doi.org/10.1145/3571730Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-111)
[^112]: Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., & Chen, W. (2022). What Makes Good In-Context Examples for GPT-3? Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. https://doi.org/10.18653/v1/2022.deelio-1.10Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-112)
[^113]: Yuan, A., Coenen, A., Reif, E., & Ippolito, D. (2022). Wordcraft: Story Writing With Large Language Models. 27th International Conference on Intelligent User Interfaces, 841â€“852.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-113)
[^114]: Fadnavis, S., Dhurandhar, A., Norel, R., Reinen, J. M., Agurto, C., Secchettin, E., Schweiger, V., Perini, G., & Cecchi, G. (2022). PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization. arXiv Preprint arXiv:2209.09814.
[â†©](https://learnprompting.org/docs/bibliography#fnref-114)
[^115]: Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., & Hajishirzi, H. (2022). Self-Instruct: Aligning Language Model with Self Generated Instructions.Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-115)
[^116]: Guo, J., Li, J., Li, D., Tiong, A. M. H., Li, B., Tao, D., & Hoi, S. C. H. (2022). From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models.
[â†©](https://learnprompting.org/docs/bibliography#fnref-116)
[^117]: Schick, T., & SchÃ¼tze, H. (2020). Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.
[â†©](https://learnprompting.org/docs/bibliography#fnref-117)
[^118]: Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., & Goldstein, T. (2023). A Watermark for Large Language Models. https://arxiv.org/abs/2301.10226Â 
[â†©](https://learnprompting.org/docs/bibliography#fnref-118)
