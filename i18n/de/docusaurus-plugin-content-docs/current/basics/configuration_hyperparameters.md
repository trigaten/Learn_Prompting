---
sidebar_position: 100
---

# üü¢ LLM Einstellungen


import Temperature from '@site/docs/assets/basics/temperature.svg';

<div style={{textAlign: 'center'}}>
  <Temperature style={{width:"100%",height:"300px",verticalAlign:"top"}}/>
</div>

# Einleitung

Die Ausgabe von LLMs kann durch *Konfigurations-Hyperparameter* beeinflusst werden, die verschiedene Aspekte des Modells steuern, z.B. wie ‚Äôzuf√§llig‚Äô es ist. Diese Hyperparameter k√∂nnen angepasst werden, um kreativere, vielf√§ltigere und interessantere Ergebnisse zu erzielen. In diesem Abschnitt werden zwei wichtige Konfigurations-Hyperparameter besprochen und wie sie die Ausgabe von LLMs beeinflussen.

:::Hinweis
[F√ºr Forscher] Diese unterscheiden sich von regul√§ren Hyperparametern wie Lernrate, Anzahl der Schichten, Gr√∂√üe der versteckten Ebenen usw.
:::

## Temperatur

Die Temperatur ist ein Konfigurations-Hyperparameter, der die Zuf√§lligkeit der Sprachmodellausgabe steuert. Eine hohe Temperatur f√ºhrt zu unvorhersehbareren und kreativeren Ergebnissen, w√§hrend eine niedrige Temperatur eine gew√∂hnlichere und konservativere Ausgabe erzeugt. Wenn du z.B. die Temperatur auf 0,5 einstellst, erzeugt das Modell in der Regel Text, der vorhersehbarer und weniger kreativ ist, als wenn du die Temperatur auf 1,0 einstellst.

## Top p

Top p, auch bekannt als Nukleus-Sampling, ist ein weiterer Konfigurations-Hyperparameter, der die Zuf√§lligkeit der Sprachmodellausgabe steuert. Er legt eine Schwellenwahrscheinlichkeit fest und w√§hlt die Top-Token aus, deren kumulative Wahrscheinlichkeit den Schwellenwert √ºberschreitet. Das Modell zieht dann zuf√§llig Stichproben aus dieser Menge von Token, um die Ausgabe zu erzeugen. Diese Methode kann zu vielf√§ltigeren und interessanteren Ergebnissen f√ºhren als herk√∂mmliche Methoden, bei denen das gesamte Vokabular nach dem Zufallsprinzip ausgew√§hlt wird. Wenn du z.B. top p auf 0,9 setzt, ber√ºcksichtigt das Modell nur die wahrscheinlichsten W√∂rter, die 90 % der Wahrscheinlichkeitsmasse ausmachen..

## Andere relevante Hyperparameter

Es gibt viele andere Hyperparameter, die die Leistung des Sprachmodells beeinflussen k√∂nnen, wie z.B. H√§ufigkeits- und Anwesenheitsstrafen. Wir behandeln sie hier nicht, werden dies aber vielleicht in Zukunft tun.

## Wie diese Hyperparameter das Ergebnis beeinflussen

Sowohl Temperatur als auch top p k√∂nnen die Ausgabe eines Sprachmodells beeinflussen, indem sie den Grad der Zuf√§lligkeit und Vielfalt im generierten Text steuern. Eine hohe Temperatur oder ein hoher top p-Wert f√ºhrt zu unvorhersehbareren und interessanteren Ergebnissen, erh√∂ht aber auch die Wahrscheinlichkeit von Fehlern oder Nonsense-Text. Eine niedrige Temperatur oder ein niedriger top p-Wert kann konservativere und vorhersehbarere Ergebnisse liefern, aber auch zu sich wiederholenden oder uninteressanten Texten f√ºhren.

Bei Aufgaben zur Texterstellung solltest du eine hohe Temperatur oder einen hohen top p-Wert verwenden. Bei Aufgaben, bei denen die Genauigkeit wichtig ist, wie z.B. bei √úbersetzungsaufgaben oder der Beantwortung von Fragen, sollte jedoch eine niedrige Temperatur oder ein niedriger top p-Wert verwendet werden, um die Genauigkeit und sachliche Richtigkeit zu verbessern.

:::Hinweis
Manchmal kann auch mehr Zuf√§lligkeit bei Aufgaben hilfreich sein, die Genauigkeit erfordern, wenn man sie mit [Speziellen Prompt Techniken](https://learnprompting.org/docs/intermediate/self_consistency) verbinded.
:::




## Fazit

Zusammenfassend l√§sst sich sagen, dass Temperatur, top p und andere Hyperparameter der Modellkonfiguration wichtige Faktoren sind, die bei der Arbeit mit Sprachmodellen zu ber√ºcksichtigen sind. Wenn man die Beziehung zwischen diesen Hyperparametern und der Modellausgabe versteht, k√∂nnen Anwender ihre Prompts f√ºr bestimmte Aufgaben und Anwendungen optimieren.

:::Warunung
Bei einigen Modellen, wie z. B. ChatGPT, kannst du diese Konfigurationshyperparameter **nicht** anpassen (es sei denn, du verwendest die API).
:::

Von jackdickens382